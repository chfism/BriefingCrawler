Question1: Your application is currently running on Amazon EC2 instances behind a load balancer. Your management has decided to use a Blue/Green deployment strategy. How should you implement this for each deployment?
 A.  Set up Amazon Route 53 health checks to fail over from any Amazon EC2 instance that is currently being deployed to.
 B.  Using AWS CloudFormation, create a test stack for validating the code, and then deploy the code to each production Amazon EC2  instance.
 C.  Create a new load balancer with new Amazon EC2 instances, carry out the deployment, and then switch DNS over to the new load  balancer using Amazon Route 53 after testing. 
 D.  Launch more Amazon EC2 instances to ensure high availability, de-register each Amazon EC2 instance from the load balancer,  upgrade it, and test it, and then register it again with the load balancer.
answers: C.


Question2: You have an application consisting of a stateless web server tier running on Amazon EC2 instances behind load balancer, and are using Amazon RDS with read replicas. Which of the following methods should you use to implement a selfhealing and cost-effective architecture? (Choose two.)
 A.  Set up a third-party monitoring solution on a cluster of Amazon EC2 instances in order to emit custom CloudWatch metrics to trigger  the termination of unhealthy Amazon EC2 instances.
 B.  Set up scripts on each Amazon EC2 instance to frequently send ICMP pings to the load balancer in order to determine which instance  is unhealthy and replace it.
 C.  Set up an Auto Scaling group for the web server tier along with an Auto Scaling policy that uses the Amazon RDS DB CPU utilization  CloudWatch metric to scale the instances.
 D.  Set up an Auto Scaling group for the web server tier along with an Auto Scaling policy that uses the Amazon EC2 CPU utilization  CloudWatch metric to scale the instances. 
 E.  Use a larger Amazon EC2 instance type for the web server tier and a larger DB instance type for the data storage layer to ensure that  they don’t become unhealthy.
 F.  Set up an Auto Scaling group for the database tier along with an Auto Scaling policy that uses the Amazon RDS read replica lag  CloudWatch metric to scale out the Amazon RDS read replicas.
 G.  Use an Amazon RDS Multi-AZ deployment.
answers: A. D.


Question3: You have a code repository that uses Amazon S3 as a data store. During a recent audit of your security controls, some concerns were raised about maintaining the integrity of the data in the Amazon S3 bucket. Another concern was raised around securely deploying code from Amazon S3 to applications running on Amazon EC2 in a virtual private cloud. What are some measures that you can implement to mitigate these concerns? (Choose two.)
 A.  Add an Amazon S3 bucket policy with a condition statement to allow access only from Amazon EC2 instances with RFC 1918 IP  addresses and enable bucket versioning.
 B.  Add an Amazon S3 bucket policy with a condition statement that requires multi-factor authentication in order to delete objects and  enable bucket versioning. 
 C.  Use a configuration management service to deploy AWS Identity and Access Management user credentials to the Amazon EC2  instances.  Use these credentials to securely access the Amazon S3 bucket when deploying code.
 D.  Create an Amazon Identity and Access Management role with authorization to access the Amazon S3 bucket, and launch all of your  application’s Amazon EC2 instances with this role. 
 E.  Use AWS Data Pipeline to lifecycle the data in your Amazon S3 bucket to Amazon Glacier on a weekly basis.
 F.  Use AWS Data Pipeline with multi-factor authentication to securely deploy code from the Amazon S3 bucket to your Amazon EC2  instances.
answers: B. 
 D.


Question4: Your company releases new features with high frequency while demanding high application availability. As part of the application’s A/B testing, logs from each updated Amazon EC2 instance of the application need to be analyzed in near real-time, to ensure that the application is working flawlessly after each deployment. If the logs show arty anomalous behavior, then the application version of the instance is changed to a more stable one. Which of the following methods should you use for shipping and analyzing the logs in a highly available manner?
 A.  Ship the logs to Amazon S3 for durability and use Amazon EMR to analyze the logs in a batch manner each hour.
 B.  Ship the logs to Amazon CloudWatch Logs and use Amazon EMR to analyze the logs in a batch manner each hour.
 C.  Ship the logs to an Amazon Kinesis stream and have the consumers analyze the logs in a live manner. 
 D.  Ship the logs to a large Amazon EC2 instance and analyze the logs in a live manner.
 E.  Store the logs locally on each instance and then have an Amazon Kinesis stream pull the logs for live analysis.
answers: C.


Question5: Your company has multiple applications running on AWS. Your company wants to develop a tool that notifies on-call teams immediately via email when an alarm is triggered in your environment. You have multiple on-cal teams that work different shifts, and the tool should handle notifying the correct teams at the correct times. How should you implement this solution?
 A.  Create an Amazon SNS topic and an Amazon SQS queue.  Configure the Amazon SQS queue as a subscriber to the Amazon SNS topic.  Configure CloudWatch alarms to notify this topic when an alarm is triggered.  Create an Amazon EC2 Auto Scaling group with both minimum and desired Instances configured to 0.  Worker nodes in this group spawn when messages are added to the queue.  Workers then use Amazon Simple Email Service to send messages to your on call teams.
 B.  Create an Amazon SNS topic and configure your on-call team email addresses as subscribers.  Use the AWS SDK tools to integrate your application with Amazon SNS and send messages to this new topic.  Notifications will be sent to on-call users when a CloudWatch alarm is triggered.
 C.  Create an Amazon SNS topic and configure your on-call team email addresses as subscribers.  Create a secondary Amazon SNS topic for alarms and configure your CloudWatch alarms to notify this topic when triggered.  Create an HTTP subscriber to this topic that notifies your application via HTTP POST when an alarm is triggered.  Use the AWS SDK tools to integrate your application with Amazon SNS and send messages to the first topic so that on-call engineers  receive alerts.
 D.  Create an Amazon SNS topic for each on-call group, and configure each of these with the team member emails as subscribers.  Create another Amazon SNS topic and configure your CloudWatch alarms to notify this topic when triggered.  Create an HTTP subscriber to this topic that notifies your application via HTTP POST when an alarm is triggered.  Use the AWS SDK tools to integrate your application with Amazon SNS and send messages to the correct team topic when on shift.
answers: D.


Question6: You are responsible for your company’s large multi-tiered Windows-based web application running on Amazon EC2 instances situated behind a load balancer. While reviewing metrics, you’ve started noticing an upwards trend for slow customer page load time. Your manager has asked you to come up with a solution to ensure that customer load time is not affected by too many requests per second. Which technique would you use to solve this issue?
 A.  Re-deploy your infrastructure using an AWS CloudFormation template.  Configure Elastic Load Balancing health checks to initiate a new AWS CloudFormation stack when health checks return failed.
 B.  Re-deploy your infrastructure using an AWS CloudFormation template.  Spin up a second AWS CloudFormation stack.  Configure Elastic Load Balancing SpillOver functionality to spill over any slow connections to the second AWS CloudFormation stack.
 C.  Re-deploy your infrastructure using AWS CloudFormation, Elastic Beanstalk, and Auto Scaling.  Set up your Auto Scaling group policies to scale based on the number of requests per second as well as the current customer load  time. 
 D.  Re-deploy your application using an Auto Scaling template.  Configure the Auto Scaling template to spin up a new Elastic Beanstalk application when the customer load time surpasses your  threshold.
answers: C.


Question7: During metric analysis, your team has determined that the company’s website is experiencing response times during peak hours that are higher than anticipated. You currently rely on Auto Scaling to make sure that you are scaling your environment during peak windows. How can you improve your Auto Scaling policy to reduce this high response time? Choose 2 answers.
 A.  Push custom metrics to CloudWatch to monitor your CPU and network bandwidth from your servers, which will allow your Auto Scaling  policy to have better fine-grain insight.
 B.  Increase your Auto Scaling group’s number of max servers. 
 C.  Create a script that runs and monitors your servers; when it detects an anomaly in load, it posts to an Amazon SNS topic that triggers  Elastic Load Balancing to add more servers to the load balancer.
 D.  Push custom metrics to CloudWatch for your application that include more detailed information about your web application, such as  how many requests it is handling and how many are waiting to be processed. 
 E.  Update the CloudWatch metric used for your Auto Scaling policy, and enable sub-minute granularity to allow auto scaling to trigger  faster.
answers: B. 
 D.


Question8: Management has reported an increase in the monthly bill from Amazon web services, and they are extremely concerned with this increased cost. Management has asked you to determine the exact cause of this increase. After reviewing the billing report, you notice an increase in the data transfer cost. How can you provide management with a better insight into data transfer use?
 A.  Update your Amazon CloudWatch metrics to use five-second granularity, which will give better detailed metrics that can be combined  with your billing data to pinpoint anomalies.
 B.  Use Amazon CloudWatch Logs to run a map-reduce on your logs to determine high usage and data transfer.
 C.  Deliver custom metrics to Amazon CloudWatch per application that breaks down application data transfer into multiple, more specific  data points. 
 D.  Using Amazon CloudWatch metrics, pull your Elastic Load Balancing outbound data transfer metrics monthly, and include them with  your billing report to show which application is causing higher bandwidth usage.
answers: C.


Question9: You currently run your infrastructure on Amazon EC2 instances behind an Auto Scaling group> All logs for you application are currently written to ephemeral storage. Recently your company experienced a major bug in code that made it through testing and was ultimately deployed to your fleet. This bug triggered your Auto Scaling group to scale up and back down before you could successfully retrieve the logs off your server to better assist you in troubleshooting the bug. Which technique should you use to make sure you are able to review your logs after your instances have shut down?
 A.  Configure the ephemeral policies on your Auto Scaling group to back up on terminate.
 B.  Configure your Auto Scaling policies to create a snapshot of all ephemeral storage on terminate.
 C.  Install the CloudWatch Logs Agent on your AMI, and configure CloudWatch Logs Agent to stream your logs. 
 D.  Install the CloudWatch monitoring agent on your AMI, and set up new SNS alert for CloudWatch metrics that triggers the CloudWatch  monitoring agent to backup all logs on the ephemeral drive.
 E.  Install the CloudWatch monitoring agent on your AMI, Update your Auto Scaling policy to enable automated CloudWatch Log copy.
answers: C.


Question10: The project you are working on currently uses a single AWS CloudFormation template to deploy its AWS infrastructure, which supports a multi-tier web application. You have been tasked with organizing the AWS CloudFormation resources so that they can be maintained in the future, and so that different departments such as Networking and Security can review the architecture before it goes to Production. How should you do this in a way that accommodates each department, using their existing workflows?
 A.  Organize the AWS CloudFormation template so that related resources are next to each other in the template, such as VPC subnets  and routing rules for Networking and security groups and IAM information for Security.
 B.  Separate the AWS CloudFormation template into a nested structure that has individual templates for the resources that are to be  governed by different departments, and use the outputs from the networking and security stacks for the application template that you  control. 
 C.  Organize the AWS CloudFormation template so that related resources are next to each other in the template for each department’s  use, leverage your existing continuous integration tool to constantly deploy changes from all parties to the Production environment,  and then run tests for validation.
 D.  Use a custom application and the AWS SDK to replicate the resources defined in the current AWS CloudFormation template, and use  the existing code review system to allow other departments to approve changes before altering the application for future deployments.
answers: B.


Question11: The operations team and the development team want a single place to view both operating system and application logs. How should you implement this using AWS services? (Choose two.)
 A.  Using AWS CloudFormation, create a CloudWatch Logs LogGroup and send the operating system and application logs of interest  using the CloudWatch Logs Agent.
 B.  Using AWS CloudFormation and configuration management, set up remote logging to send events via UDP packets to CloudTrail.
 C.  Using configuration management, set up remote logging to send events to Amazon Kinesis and insert these into Amazon CloudSearch  or Amazon Redshift, depending on available analytic tools. 
 D.  Using AWS CloudFormation, create a CloudWatch Logs LogGroup.  Because the Cloudwatch Log agent automatically sends all operating system logs, you only have to configure the application logs for  sending off-machine.
 E.  Using AWS CloudFormation, merge the application logs with the operating system logs, and use IAM Roles to allow both teams to  have access to view console output from Amazon EC2.
answers: A. C.


Question12: You have an Auto Sealing group of Instances that processes messages from an Amazon Simple Queue Service (SQS) queue. The group scales on the size of the queue. Processing Involves calling a third-party web service. The web service is complaining about the number of failed and repeated calls it is receiving from you. You have noticed that when the group scales in, instances are being terminated while they are processing. What cost-effective solution can you use to reduce the number of incomplete process attempts?
 A.  Create a new Auto Scaling group with minimum and maximum of 2 and instances running web proxy software.  Configure the VPC route table to route HTTP traffic to these web proxies.
 B.  Modify the application running on the instances to enable termination protection while it processes a task and disable it when the  processing is complete.
 C.  Increase the minimum and maximum size for the Auto Scaling group, and change the scaling policies so they scale less dynamically.
 D.  Modify the application running on the instances to put itself into an Auto Scaling Standby state while it processes a task and return  itself to InService when the processing is complete.
answers: D.


Question13: Your mobile application includes a photo-sharing service that is expecting tens of thousands of users at launch. You will leverage Amazon Simple Storage Service (S3) for storage of the user Images, and you must decide how to authenticate and authorize your users for access to these images. You also need to manage the storage of these images. Which two of the following approaches should you use? (Choose two.)
 A.  Create an Amazon S3 bucket per user, and use your application to generate the S3 URI for the appropriate content.
 B.  Use AWS Identity and Access Management (IAM) user accounts as your application-level user database, and offload the burden of  authentication from your application code.
 C.  Authenticate your users at the application level, and use AWS Security Token Service (STS) to grant token-based authorization to S3  objects. 
 D.  Authenticate your users at the application level, and send an SMS token message to the user.  Create an Amazon S3 bucket with the same name as the SMS message token, and move the user’s objects to that bucket.
 E.  Use a key-based naming scheme comprised from the user IDs for all user objects in a single Amazon S3 bucket.
answers: C. 
 E.


Question14: You are doing a load testing exercise on your application hosted on AWS. While testing your Amazon RDS MySQL DB instance, you notice that when you hit 100% CPU utilization on it, your application becomes non- responsive. Your application is read-heavy. What are methods to scale your data tier to meet the application’s needs? (Choose three.)
 A.  Add Amazon RDS DB read replicas, and have your application direct read queries to them.
 B.  Add your Amazon RDS DB instance to an Auto Scaling group and configure your CloudWatch metric based on CPU utilization.
 C.  Use an Amazon SQS queue to throttle data going to the Amazon RDS DB instance.
 D.  Use ElastiCache in front of your Amazon RDS DB to cache common queries. 
 E.  Shard your data set among multiple Amazon RDS DB instances. 
 F.  Enable Multi-AZ for your Amazon RDS DB instance.
answers: A. D. 
 E.


Question15: You are administering a continuous integration application that polls version control for changes and then launches new Amazon EC2 instances for a full suite of build tests. What should you do to ensure the lowest overall cost while being able to run as many tests in parallel as possible?
 A.  Perform syntax checking on the continuous integration system before launching a new Amazon EC2 instance for build test, unit and  integration tests.
 B.  Perform syntax and build tests on the continuous integration system before launching the new Amazon EC2 instance unit and  integration tests. 
 C.  Perform all tests on the continuous integration system, using AWS OpsWorks for unit, integration, and build tests.
 D.  Perform syntax checking on the continuous integration system before launching a new AWS Data Pipeline for coordinating the output  of unit, integration, and build tests.
answers: B.


Question16: You are using Elastic Beanstalk to manage your e-commerce store. The store is based on an open source e- commerce platform and is deployed across multiple instances in an Auto Scaling group. Your development team often creates new “extensions” for the e-commerce store. These extensions include PHP source code as well as an SQL upgrade script used to make any necessary updates to the database schema. You have noticed that some extension deployments fail due to an error when running the SQL upgrade script. After further investigation, you realize that this is because the SQL script is being executed on all of your Amazon EC2 instances. How would you ensure that the SQL script is only executed once per deployment regardless of how many Amazon EC2 instances are running at the time?
 A.  Use a “Container command” within an Elastic Beanstalk configuration file to execute the script, ensuring that the “leader only” flag is  set to true.
 B.  Make use of the Amazon EC2 metadata service to query whether the instance is marked as the leader” in the Auto Scaling group.  Only execute the script if “true” is returned.
 C.  Use a “Solo Command” within an Elastic Beanstalk configuration file to execute the script.  The Elastic Beanstalk service will ensure that the command is only executed once.
 D.  Update the Amazon RDS security group to only allow write access from a single instance in the Auto Scaling group; that way, only one  instance will successfully execute the script on the database.
answers: A.


Question17: Your current log analysis application takes more than four hours to generate a report of the top 10 users of your web application. You have been asked to implement a system that can report this information in real time, ensure that the report is always up to date, and handle increases in the number of requests to your web application. Choose the option that is cost-effective and can fulfill the requirements.
 A.  Publish your data to CloudWatch Logs, and configure your application to autoscale to handle the load on demand.
 B.  Publish your log data to an Amazon S3 bucket.  Use AWS CloudFormation to create an Auto Scaling group to scale your post-processing application which is configured to pull down  your log files stored an Amazon S3.
 C.  Post your log data to an Amazon Kinesis data stream, and subscribe your log-processing application so that is configured to process  your logging data. 
 D.  Configure an Auto Scaling group to increase the size of your Amazon EMR duster.
 E.  Create a multi-AZ Amazon RDS MySQL cluster, post the logging data to MySQL, and run a map reduce job to retrieve the required  information on user counts.
answers: C.


Question18: After reviewing the last quarter’s monthly bills, management has noticed an increase in the overall bill from Amazon. After researching this increase in cost, you discovered that one of your new services is doing a lot of GET Bucket API calls to Amazon S3 to build a metadata cache of all objects in the applications bucket. Your boss has asked you to come up with a new cost-effective way to help reduce the amount of these new GET Bucket API calls. What process should you use to help mitigate the cost?
 A.  Update your Amazon S3 buckets’ lifecycle policies to automatically push a list of objects to a new bucket, and use this list to view  objects associated with the application’s bucket.
 B.  Create a new DynamoDB table. Use the new DynamoDB table to store all metadata about all objects uploaded to Amazon S3.  Any time a new object is uploaded, update the application’s internal Amazon S3 object metadata cache from DynamoDB.
 C.  Using Amazon SNS, create a notification on any new Amazon S3 objects that automatically updates a new DynamoDB table to store  all metadata about the new object.  Subscribe the application to the Amazon SNS topic to update its internal Amazon S3 object metadata cache from the DynamoDB table. 
 D.  Upload all images to Amazon SQS, set up SQS lifecycles to move all images to Amazon S3, and initiate an Amazon SNS notification  to your application to update the application’s Internal Amazon S3 object metadata cache.
 E.  Upload all images to an ElastiCache filecache server. Update your application to now read all file metadata from the ElastiCache  filecache server, and configure the ElastiCache policies to push all files to Amazon S3 for long-term storage.
answers: C.


Question19: As part of your continuous deployment process, your application undergoes an I/O load performance test before it is deployed to production using new AMIs. The application uses one Amazon Elastic Block Store (EBS) PIOPS volume per instance and requires consistent I/O performance. Which of the following must be carried out to ensure that I/O load performance tests yield the correct results in a repeatable manner?
 A.  Ensure that the I/O block sizes for the test are randomly selected.
 B.  Ensure that the Amazon EBS volumes have been pre-warmed by reading all the blocks before the test. 
 C.  Ensure that snapshots of the Amazon EBS volumes are created as a backup.
 D.  Ensure that the Amazon EBS volume is encrypted.
 E.  Ensure that the Amazon EBS volume has been pre-warmed by creating a snapshot of the volume before the test.
answers: B.


Question20: Your development team wants account-level access to production instances in order to do live debugging of a highly secure environment. Which of the following should you do?
 A.  Place the credentials provided by Amazon Elastic Compute Cloud (EC2) into a secure Amazon Sample Storage Service (S3) bucket  with encryption enabled.  Assign AWS Identity and Access Management (IAM) users to each developer so they can download the credentials file.
 B.  Place an internally created private key into a secure S3 bucket with server-side encryption using customer keys and configuration  management, create a service account on all the instances using this private key, and assign IAM users to each developer so they can  download the file.
 C.  Place each developer’s own public key into a private S3 bucket, use instance profiles and configuration management to create a user  account for each developer on all instances, and place the user’s public keys into the appropriate account. 
 D.  Place the credentials provided by Amazon EC2 onto an MFA encrypted USB drive, and physically share it with each developer so that  the private key never leaves the office.
answers: C.


Question21: You have a complex system that involves networking, IAM policies, and multiple, three-tier applications. You are still receiving requirements for the new system, so you don’t yet know how many AWS components will be present in the final design. You want to start using AWS CloudFormation to define these AWS resources so that you can automate and version-control your infrastructure. How would you use AWS CloudFormation to provide agile new environments for your customers in a cost-effective, reliable manner?
 A.  Manually create one template to encompass all the resources that you need for the system, so you only have a single template to  version-control.
 B.  Create multiple separate templates for each logical part of the system, create nested stacks in AWS CloudFormation, and maintain  several templates to version-control. 
 C.  Create multiple separate templates for each logical part of the system, and provide the outputs from one to the next using an Amazon  Elastic Compute Cloud (EC2) instance running the SDK for finer granularity of control.
 D.  Manually construct the networking layer using Amazon Virtual Private Cloud (VPC) because this does not change often, and then use  AWS CloudFormation to define all other ephemeral resources.
answers: B.


Question22: After a daily scrum with your development teams, you’ve agreed that using Blue/Green style deployments would benefit the team. Which technique should you use to deliver this new requirement?
 A.  Re-deploy your application on AWS Elastic Beanstalk, and take advantage of Elastic Beanstalk deployment types.
 B.  Using an AWS CloudFormation template, re-deploy your application behind a load balancer, launch a new AWS CloudFormation stack  during each deployment, update your load balancer to send half your traffic to the new stack while you test, after verification update  the load balancer to send 100% of traffic to the new stack, and then terminate the old stack.
 C.  Re-deploy your application behind a load balancer that uses Auto Scaling groups, create a new identical Auto Scaling group, and  associate it to the load balancer. During deployment, set the desired number of instances on the old Auto Scaling group to zero, and  when all instances have terminated, delete the old Auto Scaling group. 
 D.  Using an AWS OpsWorks stack, re-deploy your application behind an Elastic Load Balancing load balancer and take advantage of  OpsWorks stack versioning, during deployment create a new version of your application, tell OpsWorks to launch the new version  behind your load balancer, and when the new version is launched, terminate the old OpsWorks stack.
answers: C.


Question23: You use Amazon CloudWatch as your primary monitoring system for your web application. After a recent software deployment, your users are getting Intermittent 500 Internal Server Errors when using the web application. You want to create a CloudWatch alarm, and notify an on-call engineer when these occur. How can you accomplish this using AWS services? (Choose three.)
 A.  Deploy your web application as an AWS Elastic Beanstalk application.  Use the default Elastic Beanstalk Cloudwatch metrics to capture 500 Internal Server Errors.  Set a CloudWatch alarm on that metric.
 B.  Install a CloudWatch Logs Agent on your servers to stream web application logs to CloudWatch. 
 C.  Use Amazon Simple Email Service to notify an on-call engineer when a CloudWatch alarm is triggered.
 D.  Create a CloudWatch Logs group and define metric filters that capture 500 Internal Server Errors.  Set a CloudWatch alarm on that metric. 
 E.  Use Amazon Simple Notification Service to notify an on-call engineer when a CloudWatch alarm is triggered. 
 F.  Use AWS Data Pipeline to stream web application logs from your servers to CloudWatch.
answers: B. 
 D. 
 E.


Question24: You have enabled Elastic Load Balancing HTTP health checking. After looking at the AWS Management Console, you see that all instances are passing health checks, but your customers are reporting that your site is not responding. What is the cause?
 A.  The HTTP health checking system is misreporting due to latency in inter-instance metadata synchronization.
 B.  The health check in place is not sufficiently evaluating the application function. 
 C.  The application is returning a positive health check too quickly for the AWS Management Console to respond.
 D.  Latency in DNS resolution is interfering with Amazon EC2 metadata retrieval.
answers: B.


Question25: When an Auto Scaling group is running in Amazon Elastic Compute Cloud (EC2), your application rapidly scales up and down in response to load within a 10-minute window; however, after the load peaks, you begin to see problems in your configuration management system where previously terminated Amazon EC2 resources are still showing as active. What would be a reliable and efficient way to handle the cleanup of Amazon EC2 resources within your configuration management system? (Choose two.)
 A.  Write a script that is run by a daily cron job on an Amazon EC2 instance and that executes API Describe calls of the EC2 Auto Scaling  group and removes terminated instances from the configuration management system.
 B.  Configure an Amazon Simple Queue Service (SQS) queue for Auto Scaling actions that has a script that listens for new messages  and removes terminated instances from the configuration management system.
 C.  Use your existing configuration management system to control the launching and bootstrapping of instances to reduce the number of  moving parts in the automation.
 D.  Write a small script that is run during Amazon EC2 instance shutdown to de-register the resource from the configuration management  system. 
 E.  Use Amazon Simple Workflow Service (SWF) to maintain an Amazon DynamoDB database that contains a whitelist of instances that  have been previously launched, and allow the Amazon SWF worker to remove information from the configuration management system.
answers: A. D.


Question26: You have an application running on an Amazon EC2 instance and you are using IAM roles to securely access AWS Service APIs. How can you configure your application running on that instance to retrieve the API keys for use with the AWS SDKs?
 A.  When assigning an EC2 IAM role to your instance in the console, in the “Chosen SDK” drop-down list, select the SDK that you are  using, and the instance will configure the correct SDK on launch with the API keys.
 B.  Within your application code, make a GET request to the IAM Service API to retrieve credentials for your user.
 C.  When using AWS SDKs and Amazon EC2 roles, you do not have to explicitly retrieve API keys, because the SDK handles retrieving  them from the Amazon EC2 MetaData service. 
 D.  Within your application code, configure the AWS SDK to get the API keys from environment variables, because assigning an Amazon  EC2 role stores keys in environment variables on launch.
answers: C.


Question27: You have been tasked with deploying a scalable distributed system using AWS OpsWorks. Your distributed system is required to scale on demand. As it is distributed, each node must hold a configuration file that includes the hostnames of the other instances within the layer. How should you configure AWS OpsWorks to manage scaling this application dynamically?
 A.  Create a Chef Recipe to update this configuration file, configure your AWS OpsWorks stack to use custom cookbooks, and assign this  recipe to the Configure LifeCycle Event of the specific layer.
 B.  Update this configuration file by writing a script to poll the AWS OpsWorks service API for new instances.  Configure your base AMI to execute this script on Operating System startup.
 C.  Create a Chef Recipe to update this configuration file, configure your AWS OpsWorks stack to use custom cookbooks, and assign this  recipe to execute when instances are launched.
 D.  Configure your AWS OpsWorks layer to use the AWS-provided recipe for distributed host configuration, and configure the instance  hostname and file path parameters in your recipes settings.
answers: A.


Question28: You have a large number of web servers in an Auto Scaling group behind a load balancer. On an hourly basis, you want to filter and process the logs to collect data on unique visitors, and then put that data in a durable data store in order to run reports. Web servers in the Auto Scaling group are constantly launching and terminating based on your scaling policies, but you do not want to lose any of the log data from these servers during a stop/termination initiated by a user or by Auto Scaling. What two approaches will meet these requirements? (Choose two.)
 A.  Install an Amazon Cloudwatch Logs Agent on every web server during the bootstrap process.  Create a CloudWatch log group and define Metric Filters to create custom metrics that track unique visitors from the streaming web  server logs.  Create a scheduled task on an Amazon EC2 instance that runs every hour to generate a new report based on the Cloudwatch custom  metrics.
 B.  On the web servers, create a scheduled task that executes a script to rotate and transmit the logs to Amazon Glacier.  Ensure that the operating system shutdown procedure triggers a logs transmission when the Amazon EC2 instance is  stopped/terminated.  Use Amazon Data Pipeline to process the data in Amazon Glacier and run reports every hour.
 C.  On the web servers, create a scheduled task that executes a script to rotate and transmit the logs to an Amazon S3 bucket.  Ensure that the operating system shutdown procedure triggers a logs transmission when the Amazon EC2 instance is  stopped/terminated.  Use AWS Data Pipeline to move log data from the Amazon S3 bucket to Amazon Redshift In order to process and run reports every  hour. 
 D.  Install an AWS Data Pipeline Logs Agent on every web server during the bootstrap process.  Create a log group object in AWS Data Pipeline, and define Metric Filters to move processed log data directly from the web servers to  Amazon Redshift and run reports every hour.
answers: A. C.


Question29: Your company develops a variety of web applications using many platforms and programming languages with different application dependencies. Each application must be developed and deployed quickly and be highly evadable to satisfy your business requirements. Which of the following methods should you use to deploy these applications rapidly?
 A.  Develop the applications in Docker containers, and then deploy them to Elastic Beanstalk environments with Auto Scaling and Elastic  Load Balancing.
 B.  Use the AWS CloudFormation Docker import service to build and deploy the applications with high availability in multiple Availability  Zones.
 C.  Develop each application’s code in DynamoDB, and then use hooks to deploy it to Elastic Beanstalk environments with Auto Scaling  and Elastic Load Balancing.
 D.  Store each application’s code in a Git repository, develop custom package repository managers for each application’s dependencies,  and deploy to AWS OpsWorks in multiple Availability Zones.
answers: A.


Question30: Your application uses CloudFormation to orchestrate your application’s resources. During your testing phase before the application went live, your Amazon RDS instance type was changed and caused the instance to be re-created, resulting In the loss of test data. How should you prevent this from occurring in the future?
 A.  Within the AWS CloudFormation parameter with which users can select the Amazon RDS instance type, set AllowedValues to only  contain the current instance type.
 B.  Use an AWS CloudFormation stack policy to deny updates to the instance. Only allow UpdateStack permission to IAM principals that  are denied SetStackPolicy.
 C.  In the AWS CloudFormation template, set the AWS::RDS::DBInstance’s DBlnstanceClass property to be read-only.
 D.  Subscribe to the AWS CloudFormation notification “BeforeResourceUpdate,” and call CancelStackUpdate if the resource identified is  the Amazon RDS instance.
 E.  In the AWS CloudFormation template, set the DeletionPolicy of the AWS::RDS::DBInstance’s DeletionPolicy property to “Retain.”
answers: E.


Question31: You’ve been tasked with implementing an automated data backup solution for your application servers that run on Amazon EC2 with Amazon EBS volumes. You want to use a distributed data store for your backups to avoid single points of failure and to increase the durability of the data. Daily backups should be retained for 30 days so that you can restore data within an hour. How can you implement this through a script that a scheduling daemon runs daily on the application servers?
 A.  Write the script to call the ec2-create-volume API, tag the Amazon EBS volume with the current date time group, and copy backup  data to a second Amazon EBS volume.  Use the ec2-describe-volumes API to enumerate existing backup volumes.  Call the ec2-delete-volume API to prune backup volumes that are tagged with a date-tine group older than 30 days.
 B.  Write the script to call the Amazon Glacier upload archive API, and tag the backup archive with the current date-time group.  Use the list vaults API to enumerate existing backup archives.  Call the delete vault API to prune backup archives that are tagged with a date-time group older than 30 days.
 C.  Write the script to call the ec2-create-snapshot API, and tag the Amazon EBS snapshot with the current date-time group.  Use the ec2-describe-snapshot API to enumerate existing Amazon EBS snapshots.  Call the ec2-delete-snapShot API to prune Amazon EBS snapshots that are tagged with a date-time group older than 30 days. 
 D.  Write the script to call the ec2-create-volume API, tag the Amazon EBS volume with the current date-time group, and use the ec2-  copy-snapshot API to back up data to the new Amazon EBS volume.  Use the ec2- describe-snapshot API to enumerate existing backup volumes.  Call the ec2-delete-snaphot API to prune backup Amazon EBS volumes that are tagged with a date-time group older than 30 days.
answers: C.


Question32: You work for a startup that has developed a new photo-sharing application for mobile devices. Over recent months your application has increased in popularity; this has resulted in a decrease in the performance of the application clue to the increased load. Your application has a two-tier architecture that is composed of an Auto Scaling PHP application tier and a MySQL RDS instance initially deployed with AWS Cloud Formation. Your Auto Scaling group has a min value of 4 and a max value of 8. The desired capacity is now at 8 because of the high CPU utilization of the instances. After some
 analysis, you are confident that the performance issues stem from a constraint in CPU capacity, although memory utilization remains low. You therefore decide to move from the general-purpose M3 instances to the compute-optimized C3 instances. How would you deploy this change while minimizing any interruption to your end users?
 A.  Sign into the AWS Management Console, copy the old launch configuration, and create a new launch configuration that specifies the  C3 instances.  Update the Auto Scaling group with the new launch configuration.  Auto Scaling will then update the instance type of all running instances. B.  Sign into the AWS Management Console, and update the existing launch configuration with the new C3 instance type.  Add an UpdatePolicy attribute to your Auto Scaling group that specifies AutoScalingRollingUpdate.
 C.  Update the launch configuration specified in the AWS CloudFormation template with the new C3 instance type.  Run a stack update with the new template.  Auto Scaling will then update the instances with the new instance type.
 D.  Update the launch configuration specified in the AWS CloudFormation template with the new C3 instance type.  Also add an UpdatePolicy attribute to your Auto Scaling group that specifies AutoScalingRollingUpdate.  Run a stack update with the new template.
answers: D.


Question33: You have been given a business requirement to retain log files for your application for 10 years. You need to regularly retrieve the most recent logs for troubleshooting. Your logging system must be cost-effective, given the large volume of logs. What technique should you use to meet these requirements?
 A.  Store your log in Amazon CloudWatch Logs.
 B.  Store your logs in Amazon Glacier.
 C.  Store your logs in Amazon S3, and use lifecycle policies to archive to Amazon Glacier. 
 D.  Store your logs in HDFS on an Amazon EMR cluster.
 E.  Store your logs on Amazon EBS, and use Amazon EBS snapshots to archive them.
answers: C.


Question34: You have an application running on Amazon EC2 in an Auto Scaling group. Instances are being bootstrapped dynamically, and the bootstrapping takes over 15 minutes to complete. You find that instances are reported by Auto Scaling as being In Service before bootstrapping has completed. You are receiving application alarms related to new instances before they have completed bootstrapping, which is causing confusion. You find the cause: your application monitoring tool is polling the Auto Scaling Service API for instances that are In Service, and creating alarms for new previously unknown instances. Which of the following will ensure that new instances are not added to your application monitoring tool before bootstrapping is completed?
 A.  Create an Auto Scaling group lifecycle hook to hold the instance in a pending: wait state until your bootstrapping is complete.  Once bootstrapping is complete, notify Auto Scaling to complete the lifecycle hook and move the instance into a pending: complete  state.
 B.  Use the default Amazon CloudWatch application metrics to monitor your application’s health.  Configure an Amazon SNS topic to send these CloudWatch alarms to the correct recipients.
 C.  Tag all instances on launch to identify that they are in a pending state.  Change your application monitoring tool to look for this tag before adding new instances, and the use the Amazon API to set the  instance state to ‘pending’ until bootstrapping is complete.
 D.  Increase the desired number of instances in your Auto Scaling group configuration to reduce the time it takes to bootstrap future  instances.
answers: A.


Question35: You work for an insurance company and are responsible for the day-to-day operations of your company’s online quote system used to provide insurance quotes to members of the public. Your company wants to use the application logs generated by the system to better understand customer behavior. Industry, regulations also require that you retain all application logs for the system indefinitely in order to investigate fraudulent claims in the future. You have been tasked with designing a log management system with the following requirements: – All log entries must be retained by the system, even during unplanned instance failure.
 – The customer insight team requires immediate access to the logs from the past seven days. – The fraud investigation team requires access to all historic logs, but will wait up to 24 hours before these logs are available. How would you meet these requirements in a cost-effective manner? (Choose three.)
 A.  Configure your application to write logs to the instance’s ephemeral disk, because this storage is free and has good write performance.  Create a script that moves the logs from the instance to Amazon S3 once an hour. B.  Write a script that is configured to be executed when the instance is stopped or terminated and that will upload any remaining logs on  the instance to Amazon S3.
 C.  Create an Amazon S3 lifecycle configuration to move log files from Amazon S3 to Amazon Glacier after seven days. 
 D.  Configure your application to write logs to the instance’s default Amazon EBS boot volume, because this storage already exists.  Create a script that moves the logs from the instance to Amazon S3 once an hour.
 E.  Configure your application to write logs to a separate Amazon EBS volume with the “delete on termination” field set to false.  Create a script that moves the logs from the instance to Amazon S3 once an hour. 
 F.  Create a housekeeping script that runs on a T2 micro instance managed by an Auto Scaling group for high availability.  The script uses the AWS API to identify any unattached Amazon EBS volumes containing log files.  Your housekeeping script will mount the Amazon EBS volume, upload all logs to Amazon S3, and then delete the volume.
answers: C. 
 E. 
 F.


Question36: You need to implement A/B deployments for several multi-tier web applications. Each of them has Its Individual infrastructure: Amazon Elastic Compute Cloud (EC2) front-end servers, Amazon ElastiCache clusters, Amazon Simple Queue Service (SQS) queues, and Amazon Relational Database (RDS) Instances. Which combination of services would give you the ability to control traffic between different deployed versions of your application? (Choose one.)
 A.  Create one AWS Elastic Beanstalk application and all AWS resources (using configuration files inside the application source bundle)  for each web application.  New versions would be deployed a-eating Elastic Beanstalk environments and using the Swap URLs feature.
 B.  Using AWS CloudFormation templates, create one Elastic Beanstalk application and all AWS resources (in the same template) for  each web application.  New versions would be deployed using AWS CloudFormation templates to create new Elastic Beanstalk environments, and traffic  would be balanced between them using weighted Round Robin (WRR) records in Amazon Route 53. 
 C.  Using AWS CloudFormation templates, create one Elastic Beanstalk application and all AWS resources (in the same template) for  each web application.  New versions would be deployed updating a parameter on the CloudFormation template and passing it to the cfn-hup helper daemon,  and traffic would be balanced between them using Weighted Round Robin (WRR) records in Amazon Route 53.
 D.  Create one Elastic Beanstalk application and all AWS resources (using configuration files inside the application source bundle) for  each web application.  New versions would be deployed updating the Elastic Beanstalk application version for the current Elastic Beanstalk environment.
answers: B.


Question37: Your company has developed a web application and is hosting it in an Amazon S3 bucket configured for static website hosting. The application is using the AWS SDK for JavaScript in the browser to access data stored in an Amazon DynamoDB table. How can you ensure that API keys for access to your data in DynamoDB are kept secure?
 A.  Create an Amazon S3 role in IAM with access to the specific DynamoDB tables, and assign it to the bucket hosting your website.
 B.  Configure S3 bucket tags with your AWS access keys for your bucket hosing your website so that the application can query them for  access.
 C.  Configure a web identity federation role within IAM to enable access to the correct DynamoDB resources and retrieve temporary  credentials. 
 D.  Store AWS keys in global variables within your application and configure the application to use these credentials when making requests.
answers: C.


Question38: You are using a configuration management system to manage your Amazon EC2 instances. On your Amazon EC2 Instances, you want to store credentials for connecting to an Amazon RDS DB instance. How should you securely store these credentials?
 A.  Give the Amazon EC2 instances an IAM role that allows read access to a private Amazon S3 bucket.  Store a file with database credentials in the Amazon S3 bucket.  Have your configuration management system pull the file from the bucket when it is needed.
 B.  Launch an Amazon EC2 instance and use the configuration management system to bootstrap the instance with the Amazon RDS DB  credentials.  Create an AMI from this instance.
 C.  Store the Amazon RDS DB credentials in Amazon EC2 user data.  Import the credentials into the Instance on boot.
 D.  Assign an IAM role to your Amazon RDS instance, and use this IAM role to access the Amazon RDS DB from your Amazon EC2  instances. 
 E.  Store your credentials in your version control system, in plaintext.  Check out a copy of your credentials from the version control system on boot.  Use Amazon EBS encryption on the volume storing the Amazon RDS DB credentials.
answers: D.


Question39: You want to securely distribute credentials for your Amazon RDS instance to your fleet of web server instances. The credentials are stored in a file that is controlled by a configuration management system. How do you securely deploy the credentials in an automated manner across the fleet of web server instances, which can number in the hundreds, while retaining the ability to roll back if needed?
 A.  Store your credential files in an Amazon S3 bucket.  Use Amazon S3 server-side encryption on the credential files.  Have a scheduled job that pulls down the credential files into the instances every 10 minutes.
 B.  Store the credential files in your version-controlled repository with the rest of your code.  Have a post-commit action in version control that kicks off a job in your continuous integration system which securely copses the new  credential files to all web server instances.
 C.  Insert credential files into user data and use an instance lifecycle policy to periodically refresh the file from the user data.
 D.  Keep credential files as a binary blob in an Amazon RDS MySQL DB instance, and have a script on each Amazon EC2 instance that  pulls the files down from the RDS instance. 
 E.  Store the credential files in your version-controlled repository with the rest of your code.  Use a parallel file copy program to send the credential files from your local machine to the Amazon EC2 instances.
answers: D.


Question40: Due to compliance regulations, management has asked you to provide a system that allows for cost-effective long-term storage of your application logs and provides a way for support staff to view the logs more quickly. Currently your log system archives logs automatically to Amazon S3 every hour, and support staff must wait for these logs to appear in Amazon S3, because they do not currently have access to the systems to view live logs. What method should you use to become compliant while also providing a faster way for support staff to have access to logs?
 A.  Update Amazon S3 lifecycle policies to archive old logs to Amazon Glacier, and add a new policy to push all log entries to Amazon  SQS for ingestion by the support team.
 B.  Update Amazon S3 lifecycle policies to archive old logs to Amazon Glacier, and use or write a service to also stream your application  logs to CloudWatch Logs.
 C.  Update Amazon Glacier lifecycle policies to pull new logs from Amazon S3, and in the Amazon EC2 console, enable the CloudWatch  Logs Agent on all of your application servers.
 D.  Update Amazon S3 lifecycle policies to archive old logs to Amazon Glacier. key can be different from the tableEnable Amazon S3  partial uploads on your Amazon S3 bucket, and trigger an Amazon SNS notification when a partial upload occurs.
 E.  Use or write a service to stream your application logs to CloudWatch Logs. Use an Amazon Elastic Map Reduce cluster to live stream  your logs from CloudWatch Logs for ingestion by the support team, and create a Hadoop job to push the logs to S3 in five-minute  chunks.
answers: E.


Question41: You were just hired as a DevOps Engineer for a startup. Your startup uses AWS for 100% of their infrastructure. They currently have no automation at all for deployment, and they have had many failures while trying to deploy to production. The company has told you deployment process risk mitigation is the most important thing now, and you have a lot of budget for tools and AWS resources. Their stack: 2-tier API Data stored in DynamoDB or S3, depending on type Compute layer is EC2 in Auto Scaling Groups They use Route53 for DNS pointing to an ELB An ELB balances load across the EC2 instances The scaling group properly varies between 4 and 12 EC2 servers. Which of the following approaches, given this company’s stack and their priorities, best meets the company’s needs?
 A.  Model the stack in AWS Elastic Beanstalk as a single Application with multiple Environments. Use Elastic Beanstalk’s Rolling Deploy  option to progressively roll out application code changes when promoting across environments.
 B.  Model the stack in 3 CloudFormation templates: Data layer, compute layer, and networking layer. Write stack deployment and  integration testing automation following Blue-Green methodologies.
 C.  Model the stack in AWS OpsWorks as a single Stack, with 1 compute layer and its associated ELB. Use Chef and App Deployments  to automate Rolling Deployment.
 D.  Model the stack in 1 CloudFormation template, to ensure consistency and dependency graph resolution. Write deployment and  integration testing automation following Rolling Deployment methodologies.
answers: 


Question42: You are creating a new API for video game scores. Reads are 100 times more common than writes, and the top 1% of scores are read 100 times more frequently than the rest of the scores. What’s the best design for this system, using DynamoDB?
 A.  DynamoDB table with 100x higher read than write throughput, with CloudFront caching.
 B.  DynamoDB table with roughly equal read and write throughput, with CloudFront caching.
 C.  DynamoDB table with 100x higher read than write throughput, with ElastiCache caching.
 D.  DynamoDB table with roughly equal read and write throughput, with ElastiCache caching.
answers: D.
Explanation: Because the 100x read ratio is mostly driven by a small subset, with caching, only a roughly equal number of reads to writes will miss the cache, since the supermajority will hit the top 1% scores. Knowing we need to set the values roughly equal when using caching, we select AWS ElastiCache, because CloudFront cannot directly cache DynamoDB queries, and ElastiCache is an excellent in-memory cache for database queries, rather than a distributed proxy cache for content delivery. … One solution would be to cache these reads at the application layer. Caching is a technique that is used in many high-throughput applications, offloading read activity on hot items to the cache rather than to the database. Your application can cache the most popular items in memory, or use a product such as ElastiCache to do the same. http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GuidelinesForTables.html#GuidelinesForTables. CachePopularItem

Question43: You have been asked to de-risk deployments at your company. Specifically, the CEO is concerned about outages that occur because of accidental inconsistencies between Staging and Production, which sometimes cause unexpected behaviors in Production even when Staging tests pass. You already use Docker to get high consistency between Staging and Production for the application environment on your EC2 instances. How do you further de-risk the rest of the execution environment, since in AWS, there are many service components you may use beyond EC2 virtual machines?
 A.  Develop models of your entire cloud system in CloudFormation. Use this model in Staging and Production to achieve greater parity.
 B.  Use AWS Config to force the Staging and Production stacks to have configuration parity. Any differences will be detected for you so  you are aware of risks.
 C.  Use AMIs to ensure the whole machine, including the kernel of the virual machines, is consistent, since Docker uses Linux Container  (LXC) technology, and we need to make sure the container environment is consistent.
 D.  Use AWS ECS and Docker clustering. This will make sure that the AMIs and machine sizes are the same across both environments.
answers: A.
Explanation: Only CloudFormation’s JSON Templates allow declarative version control of repeatably deployable models of entire AWS clouds. https://blogs.aws.amazon.com/application-management/blog/category/Best+practices

Question44: What is web identity federation?
 A.  Use of an identity provider like Google or Facebook to become an AWS IAM User.
 B.  Use of an identity provider like Google or Facebook to exchange for temporary AWS security credentials. 
 C.  Use of AWS IAM User tokens to log in as a Google or Facebook user.
 D.  Use of AWS STS Tokens to log in as a Google or Facebook user.
answers: B.
Explanation: … users of your app can sign in using a well-known identity provider (IdP) –such as Login with Amazon, Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP, receive an authentication token, and then exchange that token for temporary security credentials in AWS that map to an IAM role with permissions to use the resources in your AWS account. http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html

Question45: You need to create a Route53 record automatically in CloudFormation when not running in production during all launches of a Template. How should you implement this?
 A.  Use a  Parameter  for  environment , and add a  Condition  on the Route53  Resource  in the template to create the record only when  environment  is not  production .
 B.  Create two templates, one with the Route53 record value and one with a null value for the record. Use the one without it when  deploying to production.
 C.  Use a  Parameter  for  environment , and add a  Condition  on the Route53  Resource  in the template to create the record with a null string when  environment  is  production .
 D.  Create two templates, one with the Route53 record and one without it. Use the one without it when deploying to production.
answers: A.
Explanation: The best way to do this is with one template, and a Condition on the resource. Route53 does not allow null strings for records. http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html

Question46: You are designing a system which needs, at minumum, 8 m4.large instances operating to service traffic. When designing a system for high availability in the us-east-1 region, which has 6 Availability Zones, you company needs to be able to handle death of a full availability zone. How should you distribute the servers, to save as much cost as possible, assuming all of the EC2 nodes are properly linked to an ELB? Your VPC account can utilize us-east-1’s AZ’s a through f, inclusive.
 A.  3 servers in each of AZ’s a through d, inclusive.
 B.  8 servers in each of AZ’s a and b.
 C.  2 servers in each of AZ’s a through e, inclusive. 
 D.  4 servers in each of AZ’s a through c, inclusive.
answers: C.
Explanation: You need to design for N+1 redundancy on Availability Zones. ZONE_COUNT = (REQUIRED_INSTANCES / INSTANCE_COUNT_PER_ZONE) + 1. To minimize cost, spread the instances across as many possible zones as you can. By using a though e, you are allocating 5 zones. Using 2 instances, you have 10 total instances. If a single zone fails, you have 4 zones left, with 2 instances each, for a total of 8 instances. By spreading out as much as possible, you have increased cost by only 25% and significantly de-risked an availability zone failure. http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-regionsavailability-zones

Question47: You work for a company that automatically tags photographs using artificial neural networks (ANNs), which run on GPUs using C++. You receive millions of images at a time, but only 3 times per day on average. These images are loaded into an AWS S3 bucket you control for you in a batch, and then the customer publishes a JSON-formatted manifest into another S3 bucket you control as well. Each image takes 10 milliseconds to process using a full GPU. Your neural network software requires 5 minutes to bootstrap. Image tags are JSON objects, and you must publish them to an S3 bucket. Which of these is the best system architectures for this system?
 A.  Create an OpsWorks Stack with two Layers. The first contains lifecycle scripts for launching and bootstrapping an HTTP API on G2  instances for ANN image processing, and the second has an always-on instance which monitors the S3 manifest bucket for new files.  When a new file is detected, request instances to boot on the ANN layer. When the instances are booted and the HTTP APIs are up,  submit processing requests to individual instances.
 B.  Make an S3 notification configuration which publishes to AWS Lambda on the manifest bucket. Make the Lambda create a  CloudFormation Stack which contains the logic to construct an autoscaling worker tier of EC2 G2 instances with the ANN code on  each instance. Create an SQS queue of the images in the manifest. Tear the stack down when the queue is empty. 
 C.  Deploy your ANN code to AWS Lambda as a bundled binary for the C++ extension. Make an S3 notification configuration on the  manifest, which publishes to another AWS Lambda running controller code. This controller code publishes all the images in the  manifest to AWS Kinesis. Your ANN code Lambda Function uses the Kinesis as an Event Source. The system automatically scales  when the stream contains image events.
 D.  Create an Auto Scaling, Load Balanced Elastic Beanstalk worker tier Application and Environment. Deploy the ANN code to G2  instances in this tier. Set the desired capacity to 1. Make the code periodically check S3 for new manifests. When a new manifest is  detected, push all of the images in the manifest into the SQS queue associated with the Elastic Beanstalk worker tier.
answers: B.
Explanation: The Elastic Beanstalk option is incorrect because it requires a constantly-polling instance, which may break and costs money. The Lambda fleet option is incorrect because AWS Lambda does not support GPU usage. The OpsWorks stack option both requires a constantly-polling instance, and also requires complex timing and capacity planning logic. The CloudFormation option requires no polling, has no always-on instances, and allows arbitrarily fast processing by simply setting the instance count as high as needed. http://docs.aws.amazon.com/lambda/latest/dg/current-supported-versions.html

Question48: When thinking of AWS Elastic Beanstalk’s model, which is true?
 A.  Applications have many deployments, deployments have many environments.
 B.  Environments have many applications, applications have many deployments.
 C.  Applications have many environments, environments have many deployments. 
 D.  Deployments have many environments, environments have many applications.
answers: C.
Explanation: Applications group logical services. Environments belong to Applications, and typically represent different deployment levels (dev, stage, prod, fo forth). Deployments belong to environments, and are pushes of bundles of code for the environments to run. http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html

Question49: You are hired as the new head of operations for a SaaS company. Your CTO has asked you to make debugging any part of your entire operation simpler and as fast as possible. She complains that she has no idea what is going on in the complex, service-oriented architecture, because the developers just log to disk, and it’s very hard to find errors in logs on so many services. How can you best meet this requirement and satisfy your CTO?
 A.  Copy all log files into AWS S3 using a cron job on each instance. Use an S3 Notification Configuration on the  PutBucket   event and publish events to AWS Lambda. Use the Lambda to analyze logs as soon as they come in and flag issues.
 B.  Begin using CloudWatch Logs on every service. Stream all Log Groups into S3 objects. Use AWS EMR cluster jobs to perform adhoc MapReduce analysis and write new queries when needed.
 C.  Copy all log files into AWS S3 using a cron job on each instance. Use an S3 Notification Configuration on the  PutBucket   event and publish events to AWS Kinesis. Use Apache Spark on AWS EMR to perform at-scale stream processing queries on the log  chunks and flag issues.
 D.  Begin using CloudWatch Logs on every service. Stream all Log Groups into an AWS Elasticsearch Service Domain running Kibana 4  and perform log analysis on a search cluster.
answers: D.
Explanation: The Elasticsearch and Kibana 4 combination is called the ELK Stack, and is designed specifically for real-time, ad-hoc log analysis and aggregation. All other answers introduce extra delay or require pre-defined queries. Amazon Elasticsearch Service is a managed service that makes it easy to deploy, operate, and scale Elasticsearch in the AWS Cloud. Elasticsearch is a popular open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and click stream analytics. https://aws.amazon.com/elasticsearch-service/

Question50: For AWS Auto Scaling, what is the first transition state an instance enters after leaving steady state when scaling in due to health check failure or decreased load?
 A.  Terminating
 B.  Detaching
 C.  Terminating:Wait
 D.  EnteringStandby
answers: A.
Explanation: When Auto Scaling responds to a scale in event, it terminates one or more instances. These instances are detached from the Auto Scaling group and enter the Terminating state. http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/AutoScalingGroupLifecycle.html

Question51: Which of these is not an instrinsic function in AWS CloudFormation?
 A.  Fn::Equals
 B.  Fn::If
 C.  Fn::Not
 D.  Fn::Parse
answers: D.
Explanation: This is the complete list of Intrinsic Functions…: Fn::Base64, Fn::And, Fn::Equals, Fn::If, Fn::Not, Fn::Or, Fn::FindInMap, Fn::GetAtt, Fn::GetAZs, Fn::Join, Fn::Select, Ref http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html

Question52: How does Amazon RDS multi Availability Zone model work?
 A.  A second, standby database is deployed and maintained in a different availability zone from master, using synchronous replication.
 B.  A second, standby database is deployed and maintained in a different availability zone from master using asynchronous replication.
 C.  A second, standby database is deployed and maintained in a different region from master using asynchronous replication.
 D.  A second, standby database is deployed and maintained in a different region from master using synchronous replication.
answers: A.
Explanation: In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html

Question53: To monitor API calls against our AWS account by different users and entities, we can use ____ to create a history of calls in bulk for later review, and use ____ for reacting to AWS API calls in real-time.
 A.  AWS Config; AWS Inspector
 B.  AWS CloudTrail; AWS Config
 C.  AWS CloudTrail; CloudWatch Events 
 D.  AWS Config; AWS Lambda
answers: C.
Explanation: CloudTrail is a batch API call collection service, CloudWatch Events enables real-time monitoring of calls through the Rules object interface. https://aws.amazon.com/whitepapers/security-at-scale-governance-in-aws/

Question54: Your system automatically provisions EIPs to EC2 instances in a VPC on boot. The system provisions the whole VPC and stack at once. You have two of them per VPC. On your new AWS account, your attempt to create a Development environment failed, after successfully creating Staging and Production environments in the same region. What happened?
 A.  You didn’t choose the Development version of the AMI you are using.
 B.  You didn’t set the Development flag to true when deploying EC2 instances.
 C.  You hit the soft limit of 5 EIPs per region and requested a 6th. 
 D.  You hit the soft limit of 2 VPCs per region and requested a 3rd.
answers: C.
Explanation: There is a soft limit of 5 EIPs per Region for VPC on new accounts. The third environment could not allocate the 6th EIP. http://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html#limits_vpc

Question55: You are designing a service that aggregates clickstream data in batch and delivers reports to subscribers via email only once per week. Data is extremely spikey, geographically distributed, high-scale, and unpredictable. How should you design this system?
 A.  Use a large RedShift cluster to perform the analysis, and a fleet of Lambdas to perform record inserts into the RedShift tables. Lambda  will scale rapidly enough for the traffic spikes.
 B.  Use a CloudFront distribution with access log delivery to S3. Clicks should be recorded as querystring GETs to the distribution.  Reports are built and sent by periodically running EMR jobs over the access logs in S3. 
 C.  Use API Gateway invoking Lambdas which PutRecords into Kinesis, and EMR running Spark performing GetRecords on Kinesis to  scale with spikes. Spark on EMR outputs the analysis to S3, which are sent out via email.
 D.  Use AWS Elasticsearch service and EC2 Auto Scaling groups. The Autoscaling groups scale based on click throughput and stream  into the Elasticsearch domain, which is also scalable. Use Kibana to generate reports periodically.
answers: B.
Explanation: Because you only need to batch analyze, anything using streaming is a waste of money. CloudFront is a Gigabit-Scale HTTP(S) global request distribution service, so it can handle scale, geo-spread, spikes, and unpredictability. The Access Logs will contain the GET data and work just fine for batch analysis and email using EMR. Can I use Amazon CloudFront if I expect usage peaks higher than 10 Gbps or 15,000 RPS? Yes. Complete our request for higher limits here, and we will add more capacity to your account within two business days. https://aws.amazon.com/cloudfront/faqs/

Question56: You want to pass queue messages that are 1GB each. How should you achieve this?
 A.  Use Kinesis as a buffer stream for message bodies. Store the checkpoint id for the placement in the Kinesis Stream in SQS.
 B.  Use the Amazon SQS Extended Client Library for Java and Amazon S3 as a storage mechanism for message bodies. 
 C.  Use SQS’s support for message partitioning and multi-part uploads on Amazon S3.
 D.  Use AWS EFS as a shared pool storage medium. Store filesystem pointers to the files on disk in the SQS message bodies.
answers: B.
Explanation: You can manage Amazon SQS messages with Amazon S3. This is especially useful for storing and retrieving messages with a message size of up to 2 GB. To manage Amazon SQS messages with Amazon S3, use the Amazon SQS Extended Client Library for Java. http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/s3-messages.html

Question57: For AWS Auto Scaling, what is the first transition state an existing instance enters after leaving steady state in Standby mode?
 A.  Detaching
 B.  Terminating:Wait
 C.  Pending 
 D.  EnteringStandby
answers: C.
Explanation: You can put any instance that is in an InService state into a Standby state. This enables you to remove the instance from service, troubleshoot or make changes to it, and then put it back into service. Instances in a Standby state continue to be managed by the Auto Scaling group. However, they are not an active part of your application until you put them back into service. http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/AutoScalingGroupLifecycle.html

Question58: What is the scope of an EC2 EIP?
 A.  Placement Group
 B.  Availability Zone
 C.  Region 
 D.  VPC
answers: C.
Explanation: An Elastic IP address is tied to a region and can be associated only with an instance in the same region. http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/resources.html

Question59: Which status represents a failure state in AWS CloudFormation?
 A.   UPDATE_COMPLETE_CLEANUP_IN_PROGRESS
 B.   DELETE_COMPLETE_WITH_ARTIFACTS 
 C.   ROLLBACK_IN_PROGRESS 
 D.   ROLLBACK_FAILED
answers: C.
Explanation: ROLLBACK_IN_PROGRESS means an UpdateStack operation failed and the stack is in the process of trying to return to the valid, pre-update state. UPDATE_COMPLETE_CLEANUP_IN_PROGRESS means an update was successful, and CloudFormation is deleting any replaced, no longer used resources. ROLLBACK_FAILED is not a CloudFormation state (but UPDATE_ROLLBACK_FAILED is). DELETE_COMPLETE_WITH_ARTIFACTS does not exist at all. http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks.html

Question60: You need to perform ad-hoc analysis on log data, including searching quickly for specific error codes and reference numbers. Which should you evaluate first?
 A.  AWS Elasticsearch Service
 B.  AWS RedShift
 C.  AWS EMR
 D.  AWS DynamoDB
answers: A.
Explanation: Amazon Elasticsearch Service (Amazon ES) is a managed service that makes it easy to deploy, operate, and scale Elasticsearch clusters in the AWS cloud. Elasticsearch is a popular open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and click stream analytics. http://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/what-is-amazon-elasticsearch-service.html

Question61: You are building out a layer in a software stack on AWS that needs to be able to scale out to react to increased demand as fast as possible. You are running the code on EC2 instances in an Auto Scaling Group behind an ELB. Which application code deployment method should you use?
 A.  SSH into new instances that come online, and deploy new code onto the system by pulling it from an S3 bucket, which is populated  by code that you refresh from source control on new pushes.
 B.  Bake an AMI when deploying new versions of code, and use that AMI for the Auto Scaling Launch Configuration. 
 C.  Create a Dockerfile when preparing to deploy a new version to production and publish it to S3. Use UserData in the Auto Scaling  Launch configuration to pull down the Dockerfile from S3 and run it when new instances launch.
 D.  Create a new Auto Scaling Launch Configuration with UserData scripts configured to pull the latest code at all times.
answers: B.
Explanation: … the bootstrapping process can be slower if you have a complex application or multiple applications to install. Managing a fleet of applications with several build tools and dependencies can be a challenging task during rollouts. Furthermore, your deployment service should be designed to do faster rollouts to take advantage of Auto Scaling. https://d0.awsstatic.com/whitepapers/overview-of-deployment-options-on-aws.pdf

Question62: Which EBS volume type is best for high performance NoSQL cluster deployments?
 A.  io1
 B.  gp1
 C.  standard
 D.  gp2
answers: A.
Explanation: io1 volumes, or Provisioned IOPS (PIOPS) SSDs, are best for: Critical business applications that require sustained IOPS performance, or more than 10,000 IOPS or 160 MiB/s of throughput per volume, like large database workloads, such as MongoDB. http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html

Question63: You run accounting software in the AWS cloud. This software needs to be online continuously during the day every day of the week, and has a very static requirement for compute resources. You also have other, unrelated batch jobs that need to run once per day at any time of your choosing. How should you minimize cost?
 A.  Purchase a Heavy Utilization Reserved Instance to run the accounting software. Turn it off after hours.  Run the batch jobs with the same instance class, so the Reserved Instance credits are also applied to the batch jobs.
 B.  Purchase a Medium Utilization Reserved Instance to run the accounting software. Turn it off after hours.  Run the batch jobs with the same instance class, so the Reserved Instance credits are also applied to the batch jobs.
 C.  Purchase a Light Utilization Reserved Instance to run the accounting software. Turn it off after hours.  Run the batch jobs with the same instance class, so the Reserved Instance credits are also applied to the batch jobs.
 D.  Purchase a Full Utilization Reserved Instance to run the accounting software. Turn it off after hours.  Run the batch jobs with the same instance class, so the Reserved Instance credits are also applied to the batch jobs.
answers: A.
Explanation: Because the instance will always be online during the day, in a predictable manner, and there are a sequence of batch jobs to perform at any time, we should run the batch jobs when the account software is off. We can achieve Heavy Utilization by alternating these times, so we should purchase the reservation as such, as this represents the lowest cost. There is no such thing a “Full” level utilization purchases on EC2. https://d0.awsstatic.com/whitepapers/Cost_Optimization_with_AWS.pdf

Question64: What is the scope of an EC2 security group?
 A.  Availability Zone
 B.  Placement Group
 C.  Region 
 D.  VPC
answers: C.
Explanation: A security group is tied to a region and can be assigned only to instances in the same region. You can’t enable an instance to communicate with an instance outside its region using security group rules. Traffic from an instance in another region is seen as WAN bandwidth. http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/resources.html

Question65: When thinking of AWS Elastic Beanstalk, the ‘Swap Environment URLs’ feature most directly aids in what?
 A.  Immutable Rolling Deployments
 B.  Mutable Rolling Deployments
 C.  Canary Deployments
 D.  Blue-Green Deployments
answers: D.
Explanation: Simply upload the new version of your application and let your deployment service (AWS Elastic Beanstalk, AWS CloudFormation, or AWS OpsWorks) deploy a new version (green). To cut over to the new version, you simply replace the ELB URLs in your DNS records. Elastic Beanstalk has a Swap Environment URLs feature to facilitate a simpler cutover process. https://d0.awsstatic.com/whitepapers/overview-of-deployment-options-on-aws.pdf

Question66: You need to create a simple, holistic check for your system’s general availablity and uptime. Your system presents itself as an HTTP-speaking API. What is the most simple tool on AWS to achieve this with?
 A.  Route53 Health Checks
 B.  CloudWatch Health Checks
 C.  AWS ELB Health Checks
 D.  EC2 Health Checks
answers: A.
Explanation: You can create a health check that will run into perpetuity using Route53, in one API call, which will ping your service via HTTP every 10 or 30 seconds. Amazon Route 53 must be able to establish a TCP connection with the endpoint within four seconds. In addition, the endpoint must respond with an HTTP status code of 200 or greater and less than 400 within two seconds after connecting. http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-determining-health-of-endpoints.html

Question67: You need to scale an RDS deployment. You are operating at 10% writes and 90% reads, based on your logging. How best can you scale this in a simple way?
 A.  Create a second master RDS instance and peer the RDS groups.
 B.  Cache all the database responses on the read side with CloudFront.
 C.  Create read replicas for RDS since the load is mostly reads. 
 D.  Create a Multi-AZ RDS installs and route read traffic to standby.
answers: C.
Explanation: The high-availability feature is not a scaling solution for read-only scenarios; you cannot use a standby replica to serve read traffic. To service read-only traffic, you should use a Read Replica. For more information, see Working with PostgreSQL, MySQL, and MariaDB Read Replicas. http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html

Question68: You are creating an application which stores extremely sensitive financial information. All information in the system must be encrypted at rest and in transit. Which of these is a violation of this policy?
 A.  ELB SSL termination.
 B.  ELB Using Proxy Protocol v1.
 C.  CloudFront Viewer Protocol Policy set to HTTPS redirection.
 D.  Telling S3 to use AES256 on the server-side.
answers: A.
Explanation: Terminating SSL terminates the security of a connection over HTTP, removing the S for “Secure” in HTTPS. This violates the “encryption in transit” requirement in the scenario. http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/elb-listener-config.html

Question69: Fill the blanks: ____ helps us track AWS API calls and transitions, ____ helps to understand what resources we have now, and ____ allows auditing credentials and logins.
 A.  AWS Config, CloudTrail, IAM Credential Reports
 B.  CloudTrail, IAM Credential Reports, AWS Config
 C.  CloudTrail, AWS Config, IAM Credential Reports 
 D.  AWS Config, IAM Credential Reports, CloudTrail
answers: C.
Explanation: You can use AWS CloudTrail to get a history of AWS API calls and related events for your account. This includes calls made by using the AWS Management Console, AWS SDKs, command line tools, and higher-level AWS services. http://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html

Question70: You run a clustered NoSQL database on AWS EC2 using AWS EBS. You need to reduce latency for database response times. Performance is the most important concern, not availability. You did not perform the initial setup, someone without much AWS knowledge did, so you are not sure if they configured everything optimally. Which of the following is NOT likely to be an issue contributing to increased latency?
 A.  The EC2 instances are not EBS Optimized.
 B.  The database and requesting system are both in the wrong Availability Zone. 
 C.  The EBS Volumes are not using PIOPS.
 D.  The database is not running in a placement group.
answers: B.
Explanation: For the highest possible performance, all instances in a clustered database like this one should be in a single Availability Zone in a placement group, using EBS optimized instances, and using PIOPS SSD EBS Volumes. The particular Availability Zone the system is running in should not be important, as long as it is the same as the requesting resources. http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html

Question71: What is server immutability?
 A.  Not updating a server after creation.
 B.  The ability to change server counts.
 C.  Updating a server after creation.
 D.  The inability to change server counts.
answers: A.
Explanation: … disposable upgrades offer a simpler way to know if your application has unknown dependencies. The underlying EC2 instance usage is considered temporary or ephemeral in nature for the period of deployment until the current release is active. During the new release, a new set of EC2 instances are rolled out by terminating older instances. This type of upgrade technique is more common in an immutable infrastructure. https://d0.awsstatic.com/whitepapers/overview-of-deployment-options-on-aws.pdf

Question72: You are building a game high score table in DynamoDB. You will store each user’s highest score for each game, with many games, all of which have relatively similar usage levels and numbers of players. You need to be able to look up the highest score for any game. What’s the best DynamoDB key structure?
 A.  HighestScore as the hash / only key.
 B.  GameID as the hash key, HighestScore as the range key. 
 C.  GameID as the hash / only key.
 D.  GameID as the range / only key.
answers: B.
Explanation: Since access and storage for games is uniform, and you need to have ordering within each game for the scores (to access the highest value), your hash (partition) key should be the GameID, and there should be a range key for HighestScore. http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GuidelinesForTables.html#GuidelinesForTables. Partitions

Question73: You need to perform ad-hoc business analytics queries on well-structured data. Data comes in constantly at a high velocity. Your business intelligence team can understand SQL. What AWS service(s) should you look to first?
 A.  Kinesis Firehose + RDS
 B.  Kinesis Firehose + RedShift 
 C.  EMR using Hive
 D.  EMR running Apache Spark
answers: B.
Explanation: Kinesis Firehose provides a managed service for aggregating streaming data and inserting it into RedShift. RedShift also supports ad-hoc queries over well-structured data using a SQL-compliant wire protocol, so the business team should be able to adopt this system easily. https://aws.amazon.com/kinesis/firehose/details/

Question74: Your application consists of 10% writes and 90% reads. You currently service all requests through a Route53 Alias Record directed towards an AWS ELB, which sits in front of an EC2 Auto Scaling Group. Your system is getting very expensive when there are large traffic spikes during certain news events, during which many more people request to read similar data all at the same time. What is the simplest and cheapest way to reduce costs and scale with spikes like this?
 A.  Create an S3 bucket and asynchronously replicate common requests responses into S3 objects. When a request comes in for a  precomputed response, redirect to AWS S3.
 B.  Create another ELB and Auto Scaling Group layer mounted on top of the other system, adding a tier to the system. Serve most read  requests out of the top layer.
 C.  Create a CloudFront Distribution and direct Route53 to the Distribution.  Use the ELB as an Origin and specify Cache Behaviours to proxy cache requests which can be served late. 
 D.  Create a Memcached cluster in AWS ElastiCache. Create cache logic to serve requests which can be served late from the in-memory  cache for increased performance.
answers: C.
Explanation: CloudFront is ideal for scenarios in which entire requests can be served out of a cache and usage patterns involve heavy reads and spikiness in demand. A cache behavior is the set of rules you configure for a given URL pattern based on file extensions, file names, or any portion of a URL path on your website (e.g., *.jpg). You can configure multiple cache behaviors for your web distribution. Amazon CloudFront will match incoming viewer requests with your list of URL patterns, and if there is a match, the service will honor the cache behavior you configure for that URL pattern. Each cache behavior can include the following Amazon CloudFront configuration values: origin server name, viewer connection protocol, minimum expiration period, query string parameters, cookies, and trusted signers for private content. https://aws.amazon.com/cloudfront/dynamic-content/

Question75: What method should I use to author automation if I want to wait for a CloudFormation stack to finish completing in a script?
 A.  Event subscription using SQS.
 B.  Event subscription using SNS.
 C.  Poll using  ListStacks  /  list-stacks . 
 D.  Poll using  GetStackStatus  /  get-stack-status .
answers: C.
Explanation: Event driven systems are good for IFTTT logic, but only polling will make a script wait to complete. ListStacks / list-stacks is a real method, GetStackStatus / get-stack-status is not. http://docs.aws.amazon.com/cli/latest/reference/cloudformation/list-stacks.html

Question76: You have a high security requirement for your AWS accounts. What is the most rapid and sophisticated setup you can use to react to AWS API calls to your account?
 A.  Subscription to AWS Config via an SNS Topic. Use a Lambda Function to perform in-flight analysis and reactivity to changes as they  occur.
 B.  Global AWS CloudTrail setup delivering to S3 with an SNS subscription to the deliver notifications, pushing into a Lambda, which  inserts records into an ELK stack for analysis.
 C.  Use a CloudWatch Rule ScheduleExpression to periodically analyze IAM credential logs. Push the deltas for events into an ELK stack  and perform ad-hoc analysis there.
 D.  CloudWatch Events Rules which trigger based on all AWS API calls, submitting all events to an AWS Kinesis Stream for arbitrary  downstream analysis.
answers: D.
Explanation: CloudWatch Events allow subscription to AWS API calls, and direction of these events into Kinesis Streams. This allows a unified, near real-time stream for all API calls, which can be analyzed with any tool(s) of your choosing downstream. http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/EventTypes.html#api_event_type

Question77: If you’re trying to configure an AWS Elastic Beanstalk worker tier for easy debugging if there are problems finishing queue jobs, what should you configure?
 A.  Configure Rolling Deployments.
 B.  Configure Enhanced Health Reporting
 C.  Configure Blue-Green Deployments.
 D.  Configure a Dead Letter Queue
answers: D.
Explanation: Elastic Beanstalk worker environments support Amazon Simple Queue Service (SQS) dead letter queues. A dead letter queue is a queue where other (source) queues can send messages that for some reason could not be successfully processed. A primary benefit of using a dead letter queue is the ability to sideline and isolate the unsuccessfully processed messages. You can then analyze any messages sent to the dead letter queue to try to determine why they were not successfully processed. http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html#worker-deadletter

Question78: Your CTO is very worried about the security of your AWS account. How best can you prevent hackers from completely hijacking your account?
 A.  Use short but complex password on the root account and any administrators.
 B.  Use AWS IAM Geo-Lock and disallow anyone from logging in except for in your city.
 C.  Use MFA on all users and accounts, especially on the root account. 
 D.  Don’t write down or remember the root account password after creating the AWS account.
answers: C.
Explanation: For increased security, we recommend that you configure multi-factor authentication (MFA) to help protect your AWS resources. MFA adds extra security because it requires users to enter a unique authentication code from an approved authentication device or SMS text message when they access AWS websites or services. http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa.html

Question79: You need the absolute highest possible network performance for a cluster computing application. You already selected homogeneous instance types supporting 10 gigabit enhanced networking, made sure that your workload was network bound, and put the instances in a placement group. What is the last optimization you can make?
 A.  Use 9001 MTU instead of 1500 for Jumbo Frames, to raise packet body to packet overhead ratios.
 B.  Segregate the instances into different peered VPCs while keeping them all in a placement group, so each one has its own Internet  Gateway.
 C.  Bake an AMI for the instances and relaunch, so the instances are fresh in the placement group and do not have noisy neighbors.
 D.  Turn off SYN/ACK on your TCP stack or begin using UDP for higher throughput.
answers: A.
Explanation: For instances that are collocated inside a placement group, jumbo frames help to achieve the maximum network throughput possible, and they are recommended in this case. For more information, see Placement Groups. http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/network_mtu.html#jumbo_frame_instances

Question80: Your application’s Auto Scaling Group scales up too quickly, too much, and stays scaled when traffic decreases. What should you do to fix this?
 A.  Set a longer cooldown period on the Group, so the system stops overshooting the target capacity. The issue is that the scaling system  doesn’t allow enough time for new instances to begin servicing requests before measuring aggregate load again.
 B.  Calculate the bottleneck or constraint on the compute layer, then select that as the new metric, and set the metric thresholds to the  bounding values that begin to affect response latency. 
 C.  Raise the CloudWatch Alarms threshold associated with your autoscaling group, so the scaling takes more of an increase in demand  before beginning.
 D.  Use larger instances instead of lots of smaller ones, so the Group stops scaling out so much and wasting resources as the OS level,  since the OS uses a higher proportion of resources on smaller instances.
answers: B.
Explanation: Systems will always over-scale unless you choose the metric that runs out first and becomes constrained first. You also need to set the thresholds of the metric based on whether or not latency is affected by the change, to justify adding capacity instead of wasting money. http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/policy_creating.html

Question81: Your company needs to automate 3 layers of a large cloud deployment. You want to be able to track this deployment’s evolution as it changes over time, and carefully control any alterations. What is a good way to automate a stack to meet these requirements?
 A.  Use OpsWorks Stacks with three layers to model the layering in your stack.
 B.  Use CloudFormation Nested Stack Templates, with three child stacks to represent the three logical layers of your cloud. 
 C.  Use AWS Config to declare a configuration set that AWS should roll out to your cloud.
 D.  Use Elastic Beanstalk Linked Applications, passing the important DNS entires between layers using the metadata interface.
answers: B.
Explanation: Only CloudFormation allows source controlled, declarative templates as the basis for stack automation. Nested Stacks help achieve clean separation of layers while simultaneously providing a method to control all layers at once when needed. https://blogs.aws.amazon.com/application-management/post/Tx1T9JYQOS8AB9I/Use-Nested-Stacks-to-CreateReusable-Templates-and-Support-Role-Specialization

Question82: When thinking of AWS Elastic Beanstalk, which statement is true?
 A.  Worker tiers pull jobs from SNS.
 B.  Worker tiers pull jobs from HTTP.
 C.  Worker tiers pull jobs from JSON.
 D.  Worker tiers pull jobs from SQS.
answers: D.
Explanation: Elastic Beanstalk installs a daemon on each Amazon EC2 instance in the Auto Scaling group to process Amazon SQS messages in the worker environment. The daemon pulls data off the Amazon SQS queue, inserts it into the message body of an HTTP POST request, and sends it to a user-configurable URL path on the local host. The content type for the message body within an HTTP POST request is application/json by default. http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html

Question83: When thinking of AWS OpsWorks, which of the following is true?
 A.  Stacks have many layers, layers have many instances.
 B.  Instances have many stacks, stacks have many layers.
 C.  Layers have many stacks, stacks have many instances.
 D.  Layers have many instances, instances have many stacks.
answers: A.
Explanation: The stack is the core AWS OpsWorks component. It is basically a container for AWS resources–Amazon EC2 instances, Amazon RDS database instances, and so on–that have a common purpose and should be logically managed together. You define the stack’s constituents by adding one or more layers. A layer represents a set of Amazon EC2 instances that serve a particular purpose, such as serving applications or hosting a database server. An instance represents a single computing resource, such as an Amazon EC2 instance. http://docs.aws.amazon.com/opsworks/latest/userguide/welcome.html

Question84: What is true of the way that encryption works with EBS?
 A.  Snapshotting an encrypted volume makes an encrypted snapshot; restoring an encrypted snapshot creates an encrypted volume  when specified / requested.
 B.  Snapshotting an encrypted volume makes an encrypted snapshot when specified / requested; restoring an encrypted snapshot  creates an encrypted volume when specified / requested.
 C.  Snapshotting an encrypted volume makes an encrypted snapshot; restoring an encrypted snapshot always creates an encrypted  volume. 
 D.  Snapshotting an encrypted volume makes an encrypted snapshot when specified / requested; restoring an encrypted snapshot always  creates an encrypted volume.
answers: C.
Explanation: Snapshots that are taken from encrypted volumes are automatically encrypted. Volumes that are created from encrypted snapshots are also automatically encrypted. Your encrypted volumes and any associated snapshots always remain protected. For more information, see Amazon EBS Encryption. http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html

Question85: You need to run a very large batch data processing job one time per day. The source data exists entirely in S3, and the output of the processing job should also be written to S3 when finished. If you need to version control this processing job and all setup and teardown logic for the system, what approach should you use?
 A.  Model an AWS EMR job in AWS Elastic Beanstalk.
 B.  Model an AWS EMR job in AWS CloudFormation. 
 C.  Model an AWS EMR job in AWS OpsWorks.
 D.  Model an AWS EMR job in AWS CLI Composer.
answers: B.
Explanation: To declaratively model build and destroy of a cluster, you need to use AWS CloudFormation. OpsWorks and Elastic Beanstalk cannot directly model EMR Clusters. The CLI is not declarative, and CLI Composer does not exist. http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-emr-cluster.html

Question86: What is a circular dependency in AWS CloudFormation?
 A.  When a Template references an earlier version of itself.
 B.  When Nested Stacks depend on each other.
 C.  When Resources form a DependOn loop. 
 D.  When a Template references a region, which references the original Template.
answers: C.
Explanation: To resolve a dependency error, add a DependsOn attribute to resources that depend on other resources in your template. In some cases, you must explicitly declare dependencies so that AWS CloudFormation can create or delete resources in the correct order. For example, if you create an Elastic IP and a VPC with an Internet gateway in the same stack, the Elastic IP must depend on the Internet gateway attachment. For additional information, see DependsOn Attribute. http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html#troubleshooting-errorsdependence-error

Question87: Which of the following tools does not directly support AWS OpsWorks, for monitoring your stacks?
 A.  AWS Config
 B.  Amazon CloudWatch Metrics
 C.  AWS CloudTrail
 D.  Amazon CloudWatch Logs
answers: A.
Explanation: You can monitor your stacks in the following ways: AWS OpsWorks uses Amazon CloudWatch to provide thirteen custom metrics with detailed monitoring for each instance in the stack; AWS OpsWorks integrates with AWS CloudTrail to log every AWS OpsWorks API call and store the data in an Amazon S3 bucket; You can use Amazon CloudWatch Logs to monitor your stack’s system, application, and custom logs. http://docs.aws.amazon.com/opsworks/latest/userguide/monitoring.html

Question88: There is a very serious outage at AWS. EC2 is not affected, but your EC2 instance deployment scripts stopped working in the region with the outage. What might be the issue?
 A.  The AWS Console is down, so your CLI commands do not work.
 B.  S3 is unavailable, so you can’t create EBS volumes from a snapshot you use to deploy new volumes. 
 C.  AWS turns off the  DeployCode  API call when there are major outages, to protect from system floods.
 D.  None of the other answers make sense. If EC2 is not affected, it must be some other issue.
answers: B.
Explanation: S3 stores all snapshots. If S3 is unavailable, snapshots are unavailable. Amazon EC2 also uses Amazon S3 to store snapshots (backup copies) of the data volumes. You can use snapshots for recovering data quickly and reliably in case of application or system failures. You can also use snapshots as a baseline to create multiple new data volumes, expand the size of an existing data volume, or move data volumes across multiple Availability Zones, thereby making your data usage highly scalable. For more information about using data volumes and snapshots, see Amazon Elastic Block Store. http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonS3.html

Question89: Your company wants to understand where cost is coming from in the company’s production AWS account. There are a number of applications and services running at any given time. Without expending too much initial development time, how best can you give the business a good understanding of which applications cost the most per month to operate?
 A.  Create an automation script which periodically creates AWS Support tickets requesting detailed intra-month information about your bill.
 B.  Use custom CloudWatch Metrics in your system, and put a metric data point whenever cost is incurred.
 C.  Use AWS Cost Allocation Tagging for all resources which support it. Use the Cost Explorer to analyze costs throughout the month. 
 D.  Use the AWS Price API and constantly running resource inventory scripts to calculate total price based on multiplication of consumed  resources over time.
answers: C.
Explanation: Cost Allocation Tagging is a built-in feature of AWS, and when coupled with the Cost Explorer, provides a simple and robust way to track expenses. You can also use tags to filter views in Cost Explorer. Note that before you can filter views by tags in Cost Explorer, you must have applied tags to your resources and activate them, as described in the following sections. For more information about Cost Explorer, see Analyzing Your Costs with Cost Explorer. http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html

Question90: You have an asynchronous processing application using an Auto Scaling Group and an SQS Queue. The Auto Scaling Group scales according to the depth of the job queue. The completion velocity of the jobs has gone down, the Auto Scaling Group size has maxed out, but the inbound job velocity did not increase. What is a possible issue?
 A.  Some of the new jobs coming in are malformed and unprocessable.
 B.  The routing tables changed and none of the workers can process events anymore.
 C.  Someone changed the IAM Role Policy on the instances in the worker group and broke permissions to access the queue.
 D.  The scaling metric is not functioning correctly.
answers: A.
Explanation: The IAM Role must be fine, as if it were broken, NO jobs would be processed since the system would never be able to get any queue messages. The same reasoning applies to the routing table change. The scaling metric is fine, as instance count increased when the queue depth increased due to more messages entering than exiting. Thus, the only reasonable option is that some of the recent messages must be malformed and unprocessable. https://github.com/andrewtempleton/cloudacademy/blob/fca920b45234bbe99cc0e8efb9c65134884dd489/questions/null

Question91: You need your API backed by DynamoDB to stay online during a total regional AWS failure. You can tolerate a couple minutes of lag or slowness during a large failure event, but the system should recover with normal operation after those few minutes. What is a good approach?
 A.  Set up DynamoDB cross-region replication in a master-standby configuration, with a single standby in another region. Create an Auto  Scaling Group behind an ELB in each of the two regions DynamoDB is running in. Add a Route53 Latency DNS Record with DNS  Failover, using the ELBs in the two regions as the resource records.
 B.  Set up a DynamoDB Multi-Region table. Create an Auto Scaling Group behind an ELB in each of the two regions DynamoDB is  running in. Add a Route53 Latency DNS Record with DNS Failover, using the ELBs in the two regions as the resource records.
 C.  Set up a DynamoDB Multi-Region table. Create a cross-region ELB pointing to a cross-region Auto Scaling Group, and direct a  Route53 Latency DNS Record with DNS Failover to the cross-region ELB.
 D.  Set up DynamoDB cross-region replication in a master-standby configuration, with a single standby in another region. Create a crossregion ELB pointing to a cross-region Auto Scaling Group, and direct a Route53 Latency DNS Record with DNS Failover to the cross-  region ELB.
answers: A.
Explanation: There is no such thing as a cross-region ELB, nor such thing as a cross-region Auto Scaling Group, nor such thing as a DynamoDB Multi-Region Table. The only option that makes sense is the cross-regional replication version with two ELBs and ASGs with Route53 Failover and Latency DNS. http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.CrossRegionRepl.html

Question92: You need to create an audit log of all changes to customer banking data. You use DynamoDB to store this customer banking data. It’s important not to lose any information due to server failures. What is an elegant way to accomplish this?
 A.  Use a DynamoDB StreamSpecification and stream all changes to AWS Lambda. Log the changes to AWS CloudWatch Logs,  removing sensitive information before logging.
 B.  Before writing to DynamoDB, do a pre-write acknoledgment to disk on the application server, removing sensitive information before  logging. Periodically rotate these log files into S3.
 C.  Use a DynamoDB StreamSpecification and periodically flush to an EC2 instance store, removing sensitive information before putting  the objects. Periodically flush these batches to S3.
 D.  Before writing to DynamoDB, do a pre-write acknoledgment to disk on the application server, removing sensitive information before  logging. Periodically pipe these files into CloudWatch Logs.
answers: A.
Explanation: All suggested periodic options are sensitive to server failure during or between periodic flushes. Streaming to Lambda and then logging to CloudWatch Logs will make the system resilient to instance and Availability Zone failures. http://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html

Question93: Which of these is not a reason a Multi-AZ RDS instance will failover?
 A.  An Availability Zone outage
 B.  A manual failover of the DB instance was initiated using Reboot with failover
 C.  To autoscale to a higher instance class 
 D.  The primary DB instance fails
answers: C.
Explanation: The primary DB instance switches over automatically to the standby replica if any of the > following conditions occur: An Availability Zone outage, the primary DB instance fails, the DB instance’s server type is changed, the operating system of the DB instance is, undergoing software patching, a manual failover of the DB instance was initiated using Reboot with failover. http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html

Question94: Which of these configuration or deployment practices is a security risk for RDS?
 A.  Storing SQL function code in plaintext
 B.  Non-Multi-AZ RDS instance
 C.  Having RDS and EC2 instances exist in the same subnet
 D.  RDS in a public subnet
answers: D.
Explanation: Making RDS accessible to the public internet in a public subnet poses a security risk, by making your database directly addressable and spammable. DB instances deployed within a VPC can be configured to be accessible from the Internet or from EC2 instances outside the VPC. If a VPC security group specifies a port access such as TCP port 22, you would not be able to access the DB instance because the firewall for the DB instance provides access only via the IP addresses specified by the DB security groups the instance is a member of and the port defined when the DB instance was created. http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.RDSSecurityGroups.html

Question95: From a compliance and security perspective, which of these statements is true?
 A.  You do not ever need to rotate access keys for AWS IAM Users.
 B.  You do not ever need to rotate access keys for AWS IAM Roles, nor AWS IAM Users.
 C.  None of the other statements are true.
 D.  You do not ever need to rotate access keys for AWS IAM Roles.
answers: D.
Explanation: IAM Role Access Keys are auto-rotated by AWS on your behalf; you do not need to rotate them. The application is granted the permissions for the actions and resources that you’ve defined for the role through the security credentials associated with the role. These security credentials are temporary and we rotate them automatically. We make new credentials available at least five minutes prior to the expiration of the old credentials. http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html

Question96: What does it mean if you have zero IOPS and a non-empty I/O queue for all EBS volumes attached to a running EC2 instance?
 A.  The I/O queue is buffer flushing.
 B.  Your EBS disk head(s) is/are seeking magnetic stripes.
 C.  The EBS volume is unavailable. 
 D.  You need to re-mount the EBS volume in the OS.
answers: C.
Explanation: This is the definition of Unavailable from the EC2 and EBS SLA. “Unavailable” and “Unavailability” mean… For Amazon EBS, when all of your attached volumes perform zero read write IO, with pending IO in the queue. https://aws.amazon.com/ec2/sla/

Question97: If I want CloudFormation stack status updates to show up in a continuous delivery system in as close to real time as possible, how should I achieve this?
 A.  Use a long-poll on the Resources object in your CloudFormation stack and display those state changes in the UI for the system.
 B.  Use a long-poll on the  ListStacks API call for your CloudFormation stack and display those state changes in the  UI for the system.
 C.  Subscribe your continuous delivery system to an SNS topic that you also tell your CloudFormation stack to publish events into. 
 D.  Subscribe your continuous delivery system to an SQS queue that you also tell your CloudFormation stack to publish events into.
answers: C.
Explanation: Use NotificationARNs.member.N when making a CreateStack call to push stack events into SNS in nearly real-time. http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-monitor-stack.html

Question98: What is required to achieve gigabit network throughput on EC2? You already selected cluster-compute, 10GB instances with enhanced networking, and your workload is already network-bound, but you are not seeing 10 gigabit speeds.
 A.  Enable biplex networking on your servers, so packets are non-blocking in both directions and there’s no switching overhead.
 B.  Ensure the instances are in different VPCs so you don’t saturate the Internet Gateway on any one VPC.
 C.  Select PIOPS for your drives and mount several, so you can provision sufficient disk throughput.
 D.  Use a placement group for your instances so the instances are physically near each other in the same Availability Zone.
answers: D.
Explanation: You are not guaranteed 10gigabit performance, except within a placement group. A placement group is a logical grouping of instances within a single Availability Zone. Using placement groups enables applications to participate in a low-latency, 10 Gbps network. Placement groups are recommended for applications that benefit from low network latency, high network throughput, or both. http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html

Question99: You need to deploy a new application version to production. Because the deployment is high-risk, you need to roll the new version out to users over a number of hours, to make sure everything is working correctly. You need to be able to control the proportion of users seeing the new version of the application down to the percentage point. You use ELB and EC2 with Auto Scaling Groups and custom AMIs with your code pre-installed assigned to Launch Configurations. There are no database-level changes during your deployment. You have been told you cannot spend too much money, so you must not increase the number of EC2 instances much at all during the deployment, but you also need to be able to switch back to the original version of code quickly if something goes wrong. What is the best way to meet these requirements?
 A.  Create a second ELB, Auto Scaling Launch Configuration, and Auto Scaling Group using the Launch Configuration. Create AMIs with  all code pre-installed. Assign the new AMI to the second Auto Scaling Launch Configuration. Use Route53 Weighted Round Robin  Records to adjust the proportion of traffic hitting the two ELBs.
 B.  Use the Blue-Green deployment method to enable the fastest possible rollback if needed. Create a full second stack of instances and  cut the DNS over to the new stack of instances, and change the DNS back if a rollback is needed.
 C.  Create AMIs with all code pre-installed. Assign the new AMI to the Auto Scaling Launch Configuration, to replace the old one.  Gradually terminate instances running the old code (launched with the old Launch Configuration) and allow the new AMIs to boot to  adjust the traffic balance to the new code. On rollback, reverse the process by doing the same thing, but changing the AMI on the  Launch Config back to the original code.
 D.  Migrate to use AWS Elastic Beanstalk. Use the established and well-tested Rolling Deployment setting AWS provides on the new  Application Environment, publishing a zip bundle of the new code and adjusting the wait period to spread the deployment over time.  Re-deploy the old code bundle to rollback if needed.
answers: A.
Explanation: Only Weighted Round Robin DNS Records and reverse proxies allow such fine-grained tuning of traffic splits. The BlueGreen option does not meet the requirement that we mitigate costs and keep overall EC2 fleet size consistent, so we must select the 2 ELB and ASG option with WRR DNS tuning. This method is called A/B deployment and/or Canary deployment. https://d0.awsstatic.com/whitepapers/overview-of-deployment-options-on-aws.pdf

Question100: Which is not a restriction on AWS EBS Snapshots?
 A.  Snapshots which are shared cannot be used as a basis for other snapshots.
 B.  You cannot share a snapshot containing an AWS Access Key ID or AWS Secret Access Key.
 C.  You cannot share unencrypted snapshots.
 D.  Snapshot restorations are restricted to the region in which the snapshots are created.
answers: A.
Explanation: Snapshots shared with other users are usable in full by the recipient, including but limited to the ability to base modified volumes and snapshots. http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-modifying-snapshot-permissions.html

Question101: What is the order of most-to-least rapidly-scaling (fastest to scale first)? A) EC2 + ELB + Auto Scaling
 B) Lambda C) RDS
 A.  B, A, C B.  C, B, A
 C.  C, A, B
 D.  A, C, B
answers: A.
Explanation: Lambda is designed to scale instantly. EC2 + ELB + Auto Scaling require single-digit minutes to scale out. RDS will take at least 15 minutes, and will apply OS patches or any other updates when applied. https://aws.amazon.com/lambda/faqs/

Question102: Your CTO has asked you to make sure that you know what all users of your AWS account are doing to change resources at all times. She wants a report of who is doing what over time, reported to her once per week, for as broad a resource type group as possible. How should you do this?
 A.  Create a global AWS CloudTrail Trail. Configure a script to aggregate the log data delivered to S3 once per week and deliver this to  the CTO.
 B.  Use CloudWatch Events Rules with an SNS topic subscribed to all AWS API calls. Subscribe the CTO to an email type delivery on  this SNS Topic.
 C.  Use AWS IAM credential reports to deliver a CSV of all uses of IAM User Tokens over time to the CTO.
 D.  Use AWS Config with an SNS subscription on a Lambda, and insert these changes over time into a DynamoDB table. Generate  reports based on the contents of this table.
answers: A.
Explanation: This is the ideal use case for AWS CloudTrail. CloudTrail provides visibility into user activity by recording API calls made on your account. CloudTrail records important information about each API call, including the name of the API, the identity of the caller, the time of the API call, the request parameters, and the response elements returned by the AWS service. This information helps you to track changes made to your AWS resources and to troubleshoot operational issues. CloudTrail makes it easier to ensure compliance with internal policies and regulatory standards. https://aws.amazon.com/cloudtrail/faqs/

Question103: You are building a mobile app for consumers to post cat pictures online. You will be storing the images in AWS S3. You want to run the system very cheaply and simply. Which one of these options allows you to build a photo sharing application without needing to worry about scaling expensive uploads processes, authentication/authorization and so forth?
 A.  Build the application out using AWS Cognito and web identity federation to allow users to log in using Facebook or Google Accounts.  Once they are logged in, the secret token passed to that user is used to directly access resources on AWS, like AWS S3.
 B.  Use JWT or SAML compliant systems to build authorization policies.  Users log in with a username and password, and are given a token they can use indefinitely to make calls against the photo  infrastructure.
 C.  Use AWS API Gateway with a constantly rotating API Key to allow access from the client-side.  Construct a custom build of the SDK and include S3 access in it.
 D.  Create an AWS oAuth Service Domain ad grant public signup and access to the domain.  During setup, add at least one major social media site as a trusted Identity Provider for users.
answers: A.
Explanation: The short answer is that Amazon Cognito is a superset of the functionality provided by web identity federation. It supports the same providers, and you configure your app and authenticate with those providers in the same way. But Amazon Cognito includes a variety of additional features. For example, it enables your users to start using the app as a guest user and later sign in using one of the supported identity providers. https://blogs.aws.amazon.com/security/post/Tx3SYCORF5EKRC0/How-Does-Amazon-Cognito-Relate-to-ExistingWeb-Identity-Federatio

Question104: What is the scope of AWS IAM?
 A.  Global
 B.  Availability Zone
 C.  Region
 D.  Placement Group
answers: A.
Explanation: IAM resources are all global; there is not regional constraint. https://aws.amazon.com/iam/faqs/

Question105: You are building a Ruby on Rails application for internal, non-production use which uses MySQL as a database. You want developers without very much AWS experience to be able to deploy new code with a single command line push. You also want to set this up as simply as possible. Which tool is ideal for this setup?
 A.  AWS CloudFormation
 B.  AWS OpsWorks
 C.  AWS ELB + EC2 with CLI Push
 D.  AWS Elastic Beanstalk
answers: D.
Explanation: Elastic Beanstalk’s primary mode of operation exactly supports this use case out of the box. It is simpler than all the other options for this question. With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS cloud without worrying about the infrastructure that runs those applications. AWS Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring. http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_Ruby_rails.html

Question106: You need to replicate API calls across two systems in real time. What tool should you use as a buffer and transport mechanism for API call events?
 A.  AWS SQS
 B.  AWS Lambda
 C.  AWS Kinesis 
 D.  AWS SNS
answers: C.
Explanation: AWS Kinesis is an event stream service. Streams can act as buffers and transport across systems for in-order programmatic events, making it ideal for replicating API calls across systems. A typical Amazon Kinesis Streams application reads data from an Amazon Kinesis stream as data records. These applications can use the Amazon Kinesis Client Library, and they can run on Amazon EC2 instances. The processed records can be sent to dashboards, used to generate alerts, dynamically change pricing and advertising strategies, or send data to a variety of other AWS services. For information about Streams features and pricing, see Amazon Kinesis Streams. http://docs.aws.amazon.com/kinesis/latest/dev/introduction.html

Question107: Your team wants to begin practicing continuous delivery using CloudFormation, to enable automated builds and deploys of whole, versioned stacks or stack layers. You have a 3-tier, mission-critical system. Which of the following is NOT a best practice for using CloudFormation in a continuous delivery environment?
 A.  Use the AWS CloudFormation  ValidateTemplate  call before publishing changes to AWS.
 B.  Model your stack in one template, so you can leverage CloudFormation’s state management and dependency resolution to propagate  all changes. 
 C.  Use CloudFormation to create brand new infrastructure for all stateless resources on each push, and run integration tests on that set  of infrastructure.
 D.  Parametrize the template and use  Mappings  to ensure your template works in multiple Regions.
answers: B.
Explanation: Putting all resources in one stack is a bad idea, since different tiers have different life cycles and frequencies of change. For additional guidance about organizing your stacks, you can use two common frameworks: a multi-layered architecture and service-oriented architecture (SOA). http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#organizingstacks

Question108: Which of these is not a CloudFormation Helper Script?
 A.  cfn-signal
 B.  cfn-hup
 C.  cfn-request 
 D.  cfn-get-metadata
answers: C.
Explanation: This is the complete list of CloudFormation Helper Scripts: cfn-init, cfn-signal, cfn-get-metadata, cfn-hup http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-helper-scripts-reference.html

Question109: When thinking of AWS OpsWorks, which of the following is not an instance type you can allocate in a stack layer?
 A.  24/7 instances
 B.  Spot instances 
 C.  Time-based instances
 D.  Load-based instances
answers: B.
Explanation: AWS OpsWorks supports the following instance types, which are characterized by how they are started and stopped. 24/7 instances are started manually and run until you stop them.Time-based instances are run by AWS OpsWorks on a specified daily and weekly schedule. They allow your stack to automatically adjust the number of instances to accommodate predictable usage patterns. Load-based instances are automatically started and stopped by AWS OpsWorks, based on specified load metrics, such as CPU utilization. They allow your stack to automatically adjust the number of instances to accommodate variations in incoming traffic. Load-based instances are available only for Linuxbased stacks. http://docs.aws.amazon.com/opsworks/latest/userguide/welcome.html

Question110: You run a 2000-engineer organization. You are about to begin using AWS at a large scale for the first time. You want to integrate with your existing identity management system running on Microsoft Active Directory, because your organization is a power-user of Active Directory. How should you manage your AWS identities in the most simple manner?
 A.  Use a large AWS Directory Service Simple AD.
 B.  Use a large AWS Directory Service AD Connector. 
 C.  Use an Sync Domain running on AWS Directory Service.
 D.  Use an AWS Directory Sync Domain running on AWS Lambda.
answers: B.
Explanation: You must use AD Connector as a power-user of Microsoft Active Directory. Simple AD only works with a subset of AD functionality. Sync Domains do not exist; they are made up answers. AD Connector is a directory gateway that allows you to proxy directory requests to your on-premises Microsoft Active Directory, without caching any information in the cloud. AD Connector comes in 2 sizes; small and large. A small AD Connector is designed for smaller organizations of up to 500 users. A large AD Connector is designed for larger organizations of up to 5,000 users. https://aws.amazon.com/directoryservice/details/

Question111: You need to deploy an AWS stack in a repeatable manner across multiple environments. You have selected CloudFormation as the right tool to accomplish this, but have found that there is a resource type you need to create and model, but is unsupported by CloudFormation. How should you overcome this challenge?
 A.  Use a CloudFormation Custom Resource Template by selecting an API call to proxy for create, update, and delete actions.  CloudFormation will use the AWS SDK, CLI, or API method of your choosing as the state transition function for the resource type you  are modeling.
 B.  Submit a ticket to the AWS Forums. AWS extends CloudFormation Resource Types by releasing tooling to the AWS Labs organization  on GitHub. Their response time is usually 1 day, and they complete requests within a week or two.
 C.  Instead of depending on CloudFormation, use Chef, Puppet, or Ansible to author Heat templates, which are declarative stack resource  definitions that operate over the OpenStack hypervisor and cloud environment.
 D.  Create a CloudFormation Custom Resource Type by implementing create, update, and delete functionality, either by subscribing a  Custom Resource Provider to an SNS topic, or by implementing the logic in AWS Lambda.
answers: D.
Explanation: Custom resources provide a way for you to write custom provisioning logic in AWS CloudFormation template and have AWS CloudFormation run it during a stack operation, such as when you create, update or delete a stack. For more information, see Custom Resources. http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html

Question112: You are designing an enterprise data storage system. Your data management software system requires mountable disks and a real filesystem, so you cannot use S3 for storage. You need persistence, so you will be using AWS EBS Volumes for your system. The system needs as low-cost storage as possible, and access is not frequent or high throughput, and is mostly sequential reads. Which is the most appropriate EBS Volume Type for this scenario?
 A.  gp1
 B.  io1
 C.  standard 
 D.  gp2
answers: C.
Explanation: standard volumes, or Magnetic volumes, are best for: Cold workloads where data is infrequently accessed, or scenarios where the lowest storage cost is important. http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html

Question113: Your API requires the ability to stay online during AWS regional failures. Your API does not store any state, it only aggregates data from other sources – you do not have a database. What is a simple but effective way to achieve this uptime goal?
 A.  Use a CloudFront distribution to serve up your API. Even if the region your API is in goes down, the edge locations CloudFront uses  will be fine.
 B.  Use an ELB and a cross-zone ELB deployment to create redundancy across datacenters. Even if a region fails, the other AZ will stay  online.
 C.  Create a Route53 Weighted Round Robin record, and if one region goes down, have that region redirect to the other region.
 D.  Create a Route53 Latency Based Routing Record with Failover and point it to two identical deployments of your stateless API in two  different regions. Make sure both regions use Auto Scaling Groups behind ELBs.
answers: D.
Explanation: Latency Based Records allow request distribution when all is well with both regions, and the Failover component enables fallbacks between regions. By adding in the ELB and ASG, your system in the surviving region can expand to meet 100% of demand instead of the original fraction, whenever failover occurs. http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html

Question114: For AWS CloudFormation, which is true?
 A.  Custom resources using SNS have a default timeout of 3 minutes.
 B.  Custom resources using SNS do not need a  ServiceToken  property.
 C.  Custom resources using Lambda and  Code.ZipFile  allow inline nodejs resource composition. 
 D.  Custom resources using Lambda do not need a  ServiceToken property.
answers: C.
Explanation: Code is a property of the AWS::Lambda::Function resource that enables to you specify the source code of an AWS Lambda (Lambda) function. You can point to a file in an Amazon Simple Storage Service (Amazon S3) bucket or specify your source code as inline text (for nodejs runtime environments only). http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html

Question115: Which of these is not an intrinsic function in AWS CloudFormation?
 A.  Fn::Split
 B.  Fn::FindInMap
 C.  Fn::Select
 D.  Fn::GetAZs
answers: A.
Explanation: This is the complete list of Intrinsic Functions: Fn::Base64, Fn::And, Fn::Equals, Fn::If, Fn::Not, Fn::Or, Fn::FindInMap, Fn::GetAtt, Fn::GetAZs, Fn::Join, Fn::Select, Ref. http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html

Question116: You meet once per month with your operations team to review the past month’s data. During the meeting, you realize that 3 weeks ago, your monitoring system which pings over HTTP from outside AWS recorded a large spike in latency on your 3-tier web service API. You use DynamoDB for the database layer, ELB, EBS, and EC2 for the business logic tier, and SQS, ELB, and EC2 for the presentation layer. Which of the following techniques will NOT help you figure out what happened?
 A.  Check your CloudTrail log history around the spike’s time for any API calls that caused slowness.
 B.  Review CloudWatch Metrics graphs to determine which component(s) slowed the system down. 
 C.  Review your ELB access logs in S3 to see if any ELBs in your system saw the latency.
 D.  Analyze your logs to detect bursts in traffic at that time.
answers: B.
Explanation: Metrics data are available for 2 weeks. If you want to store metrics data beyond that duration, you can retrieve it using our GetMetricStatistics API as well as a number of applications and tools offered by AWS partners. https://aws.amazon.com/cloudwatch/faqs/

Question117: You are experiencing performance issues writing to a DynamoDB table. Your system tracks high scores for video games on a marketplace. Your most popular game experiences all of the performance issues. What is the most likely problem?
 A.  DynamoDB’s vector clock is out of sync, because of the rapid growth in request for the most popular game.
 B.  You selected the Game ID or equivalent identifier as the primary partition key for the table. 
 C.  Users of the most popular video game each perform more read and write requests than average.
 D.  You did not provision enough read or write throughput to the table.
answers: B.
Explanation: The primary key selection dramatically affects performance consistency when reading or writing to DynamoDB. By selecting a key that is tied to the identity of the game, you forced DynamoDB to create a hotspot in the table partitions, and over-request against the primary key partition for the popular game. When it stores data, DynamoDB divides a table’s items into multiple partitions, and distributes the data primarily based upon the partition key value. The provisioned throughput associated with a table is also divided evenly among the partitions, with no sharing of provisioned throughput across partitions. http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GuidelinesForTables.html#GuidelinesForTables. UniformWorkload

Question118: What is the scope of an EBS volume?
 A.  VPC
 B.  Region
 C.  Placement Group
 D.  Availability Zone
answers: D.
Explanation: An Amazon EBS volume is tied to its Availability Zone and can be attached only to instances in the same Availability Zone. http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/resources.html

Question119: Which of these is not a Pseudo Parameter in AWS CloudFormation?
 A.  AWS::StackName
 B.  AWS::AccountId
 C.  AWS::StackArn 
 D.  AWS::NotificationARNs
answers: C.
Explanation: This is the complete list of Pseudo Parameters: AWS::AccountId, AWS::NotificationARNs, AWS::NoValue, AWS::Region, AWS::StackId, AWS::StackName http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudo-parameter-reference.html

Question120: Your CTO thinks your AWS account was hacked. What is the only way to know for certain if there was unauthorized access and what they did, assuming your hackers are very sophisticated AWS engineers and doing everything they can to cover their tracks?
 A.  Use CloudTrail Log File Integrity Validation.
 B.  Use AWS Config SNS Subscriptions and process events in real time.
 C.  Use CloudTrail backed up to AWS S3 and Glacier.
 D.  Use AWS Config Timeline forensics.
answers: A.
Explanation: You must use CloudTrail Log File Validation (default or custom implementation), as any other tracking method is subject to forgery in the event of a full account compromise by sophisticated enough hackers. Validated log files are invaluable in security and forensic investigations. For example, a validated log file enables you to assert positively that the log file itself has not changed, or that particular user credentials performed specific API activity. The CloudTrail log file integrity validation process also lets you know if a log file has been deleted or changed, or assert positively that no log files were delivered to your account during a given period of time. http://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html

Question121: You need to migrate 10 million records in one hour into DynamoDB. All records are 1.5KB in size. The data is evenly distributed across the partition key. How many write capacity units should you provision during this batch load?
 A.  6667
 B.  4166
 C.  5556 
 D.  2778
answers: C.
Explanation: You need 2 units to make a 1.5KB write, since you round up. You need 20 million total units to perform this load. You have 3600 seconds to do so. Divide and round up for 5556. http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html

Question122: For AWS CloudFormation, which stack state refuses UpdateStack calls?
 A.   UPDATE_ROLLBACK_FAILED
 B.   UPDATE_ROLLBACK_COMPLETE 
 C.   UPDATE_COMPLETE 
 D.   CREATE_COMPLETE
answers: A.
Explanation: When a stack is in the UPDATE_ROLLBACK_FAILED state, you can continue rolling it back to return it to a working state (to UPDATE_ROLLBACK_COMPLETE). You cannot update a stack that is in the UPDATE_ROLLBACK_FAILED state. However, if you can continue to roll it back, you can return the stack to its original settings and try to update it again. http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stackscontinueupdaterollback.html

Question123: Why are more frequent snapshots or EBS Volumes faster?
 A.  Blocks in EBS Volumes are allocated lazily, since while logically separated from other EBS Volumes, Volumes often share the same  physical hardware. Snapshotting the first time forces full block range allocation, so the second snapshot doesn’t need to perform the  allocation phase and is faster.
 B.  The snapshots are incremental so that only the blocks on the device that have changed after your last snapshot are saved in the new  snapshot. 
 C.  AWS provisions more disk throughput for burst capacity during snapshots if the drive has been pre-warmed by snapshotting and  reading all blocks.
 D.  The drive is pre-warmed, so block access is more rapid for volumes when every block on the device has already been read at least  one time.
answers: B.
Explanation: After writing data to an EBS volume, you can periodically create a snapshot of the volume to use as a baseline for new volumes or for data backup. If you make periodic snapshots of a volume, the snapshots are incremental so that only the blocks on the device that have changed after your last snapshot are saved in the new snapshot. Even though snapshots are saved incrementally, the snapshot deletion process is designed so that you need to retain only the most recent snapshot in order to restore the volume. http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html

Question124: Your serverless architecture using AWS API Gateway, AWS Lambda, and AWS DynamoDB experienced a large increase in traffic to a sustained 400 requests per second, and dramatically increased in failure rates. Your requests, during normal operation, last 500 milliseconds on average. Your DynamoDB table did not exceed 50% of provisioned throughput, and
 Table primary keys are designed correctly. What is the most likely issue?
 A.  Your API Gateway deployment is throttling your requests. B.  Your AWS API Gateway Deployment is bottlenecking on request (de)serialization.
 C.  You did not request a limit increase on concurrent Lambda function executions. 
 D.  You used Consistent Read requests on DynamoDB and are experiencing semaphore lock.
answers: C.
Explanation: AWS API Gateway by default throttles at 500 requests per second steady-state, and 1000 requests per second at spike. Lambda, by default, throttles at 100 concurrent requests for safety. At 500 milliseconds (half of a second) per request, you can expect to support 200 requests per second at 100 concurrency. This is less than the 400 requests per second your system now requires. Make a limit increase request via the AWS Support Console. AWS Lambda: Concurrent requests safety throttle per account -> 100 http://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html#limits_lambda

Question125: You need to grant a vendor access to your AWS account. They need to be able to read protected messages in a private S3 bucket at their leisure. They also use AWS. What is the best way to accomplish this?
 A.  Create an IAM User with API Access Keys. Grant the User permissions to access the bucket. Give the vendor the AWS Access Key  ID and AWS Secret Access Key for the User.
 B.  Create an EC2 Instance Profile on your account. Grant the associated IAM role full access to the bucket. Start an EC2 instance with  this Profile and give SSH access to the instance to the vendor.
 C.  Create a cross-account IAM Role with permission to access the bucket, and grant permission to use the Role to the vendor AWS  account. 
 D.  Generate a signed S3 PUT URL and a signed S3 PUT URL, both with wildcard values and 2 year durations. Pass the URLs to the  vendor.
answers: C.
Explanation: When third parties require access to your organization’s AWS resources, you can use roles to delegate access to them. For example, a third party might provide a service for managing your AWS resources. With IAM roles, you can grant these third parties access to your AWS resources without sharing your AWS security credentials. Instead, the third party can access your AWS resources by assuming a role that you create in your AWS account. http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_third-party.html

Question126: You need to know when you spend $1000 or more on AWS. What’s the easy way for you to see that notification?
 A.  AWS CloudWatch Events tied to API calls, when certain thresholds are exceeded, publish to SNS.
 B.  Scrape the billing page periodically and pump into Kinesis.
 C.  AWS CloudWatch Metrics + Billing Alarm + Lambda event subscription. When a threshold is exceeded, email the manager. 
 D.  Scrape the billing page periodically and publish to SNS.
answers: C.
Explanation: Even if you’re careful to stay within the free tier, it’s a good idea to create a billing alarm to notify you if you exceed the limits of the free tier. Billing alarms can help to protect you against unknowingly accruing charges if you inadvertently use a service outside of the free tier or if traffic exceeds your expectations. http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/free-tier-alarms.html

Question127: You are getting a lot of empty receive requests when using Amazon SQS. This is making a lot of unnecessary network load on your instances. What can you do to reduce this load?
 A.  Subscribe your queue to an SNS topic instead.
 B.  Use as long of a poll as possible, instead of short polls. 
 C.  Alter your visibility timeout to be shorter.
 D.  Use  sqsd  on your EC2 instances.
answers: B.
Explanation: One benefit of long polling with Amazon SQS is the reduction of the number of empty responses, when there are no messages available to return, in reply to a ReceiveMessage request sent to an Amazon SQS queue. Long polling allows the Amazon SQS service to wait until a message is available in the queue before sending a response. http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html

Question128: You need to process long-running jobs once and only once. How might you do this?
 A.  Use an SNS queue and set the visibility timeout to long enough for jobs to process.
 B.  Use an SQS queue and set the reprocessing timeout to long enough for jobs to process.
 C.  Use an SQS queue and set the visibility timeout to long enough for jobs to process. 
 D.  Use an SNS queue and set the reprocessing timeout to long enough for jobs to process.
answers: C.
Explanation: The message timeout defines how long after a successful receive request SQS waits before allowing jobs to be seen by other components, and proper configuration prevents duplicate processing. http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/MessageLifecycle.html

Question129: When thinking of DynamoDB, what are true of Global Secondary Key properties?
 A.  The partition key and sort key can be different from the table.
 B.  Only the partition key can be different from the table.
 C.  Either the partition key or the sort key can be different from the table, but not both.
 D.  Only the sort key can be different from the table.
answers: A.
Explanation: Global secondary index — an index with a partition key and a sort key that can be different from those on the table. A global secondary index is considered “global” because queries on the index can span all of the data in a table, across all partitions. http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html

Question130: You need your CI to build AMIs with code pre-installed on the images on every new code push. You need to do this as cheaply as possible. How do you do this?
 A.  Bid on spot instances just above the asking price as soon as new commits come in, perform all instance configuration and setup, then  create an AMI based on the spot instance.
 B.  Have the CI launch a new on-demand EC2 instance when new commits come in, perform all instance configuration and setup, then  create an AMI based on the on-demand instance.
 C.  Purchase a Light Utilization Reserved Instance to save money on the continuous integration machine.  Use these credits whenever your create AMIs on instances.
 D.  When the CI instance receives commits, attach a new EBS volume to the CI machine. Perform all setup on this EBS volume so you  don’t need a new EC2 instance to create the AMI.
answers: A.
Explanation: Spot instances are the cheapest option, and you can use minimum run duration if your AMI takes more than a few minutes to create. Spot instances are also available to run for a predefined duration – in hourly increments up to six hours in length – at a significant discount (30-45%) compared to On-Demand pricing plus an additional 5% during off-peak times1 for a total of up to 50% savings. https://aws.amazon.com/ec2/spot/pricing/

Question1: Your application is currently running on Amazon EC2 instances behind a load balancer. Your management has decided to use a Blue/Green deployment strategy. How should you implement this for each deployment?
 A.  Set up Amazon Route 53 health checks to fail over from any Amazon EC2 instance that is currently being deployed to.
 B.  Using AWS CloudFormation, create a test stack for validating the code, and then deploy the code to each production Amazon EC2  instance.
 C.  Create a new load balancer with new Amazon EC2 instances, carry out the deployment, and then switch DNS over to the new load  balancer using Amazon Route 53 after testing. 
 D.  Launch more Amazon EC2 instances to ensure high availability, de-register each Amazon EC2 instance from the load balancer,  upgrade it, and test it, and then register it again with the load balancer.
answers: C.


